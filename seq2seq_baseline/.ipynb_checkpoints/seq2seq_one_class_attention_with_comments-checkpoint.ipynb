{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtokentome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/25/e2f9863b78e5aef61bc0475bfac39f56197103f767e6f2e957cc67b989f2/youtokentome-1.0.5.tar.gz (86kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 9.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Click>=7.0 (from youtokentome)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 3.8MB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: youtokentome\n",
      "  Running setup.py bdist_wheel for youtokentome ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/30/65/29/959c7ddc32cd220a9e633c5818802efe41a990c476220aed69\n",
      "Successfully built youtokentome\n",
      "Installing collected packages: Click, youtokentome\n",
      "Successfully installed Click-7.0 youtokentome-1.0.5\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install youtokentome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import youtokentome as yttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            context  \\\n",
       "0           0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1           1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2           2  Managed by her father, Mathew Knowles, the gro...   \n",
       "3           3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4           4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question  \n",
       "0           When did Beyonce start becoming popular?  \n",
       "1  What areas did Beyonce compete in when she was...  \n",
       "2  When did Beyonce leave Destiny's Child and bec...  \n",
       "3      In what city and state did Beyonce  grow up?   \n",
       "4         In which decade did Beyonce become famous?  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ИЗМЕНИТЬ ПУТЬ В ЗАВИСИМОСТИ ОТ ТОГО ГДЕ ЗАПУСКАЕМ КОД\n",
    "try:\n",
    "    df = pd.read_csv(\"ctx_quest.csv\")\n",
    "except:\n",
    "    df = pd.read_csv(\"/Users/lilyakhoang/input/question_generation/ctx_quest.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 86610 entries, 0 to 86820\n",
      "Data columns (total 3 columns):\n",
      "Unnamed: 0    86610 non-null int64\n",
      "context       86610 non-null object\n",
      "question      86610 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_nonan = df.dropna()\n",
    "df_nonan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # для \"обучения\" bpe модели нам нужно сохранить данные для обучения в отдельный файл\n",
    "# # где будут построчно храниться тексты\n",
    "# err = 0\n",
    "# try:\n",
    "#     f = open('/Users/lilyakhoang/input/for_bpe_ctx_quest.txt', 'w')\n",
    "#     bpe_model_address = '/Users/lilyakhoang/input/for_bpe_ctx_quest.txt'\n",
    "# except:\n",
    "#     f = open('for_bpe_ctx_quest.txt', 'w')\n",
    "#     bpe_model_address = 'for_bpe_ctx_quest.txt'\n",
    "# for que in df_nonan.context:\n",
    "#     try:\n",
    "#         f.write(que + '\\n')\n",
    "#     except:\n",
    "#         err += 1\n",
    "# for que in df_nonan.question:\n",
    "#     try:\n",
    "#         f.write(que + '\\n')\n",
    "#     except:\n",
    "#         err += 1\n",
    "# f.close()\n",
    "# print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем\n",
    "vocab_size = 16000\n",
    "model_path = 'bpe.model'\n",
    "\n",
    "# yttm.BPE.train(data='for_bpe_ctx_quest.txt', vocab_size=vocab_size, model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(model=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Убрал BOS и EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3ca1eeb6a549d4ba0be24633278ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=339.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171f5139ddb24384a14e36e28ed2c3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=339.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# давайте токенизируем наш датасет\n",
    "# токенизирую батчами, потому что так быстрее\n",
    "# также в начало добавляем токен bos (begin of sentence)\n",
    "\n",
    "tokenized_ctx = []\n",
    "tokenized_quest = []\n",
    "batch_size = 256\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(df_nonan.context) / batch_size))):\n",
    "    \n",
    "    tokenized_ctx.extend(tokenizer.encode(list(df_nonan.context[i_batch*batch_size:(i_batch+1)*batch_size])))\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(df_nonan.question) / batch_size))):\n",
    "\n",
    "    tokenized_quest.extend(tokenizer.encode(list(df_nonan.question[i_batch*batch_size:(i_batch+1)*batch_size])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример с переводом:\n",
    "1. что подаем на вход энкодеру: мама мыла раму\n",
    "1. что подаем на вход декодеру: bos mom was washing the frame\n",
    "1. наши таргеты (от чего считать лосс): mom was washing the frame eos\n",
    "\n",
    "То есть наши таргеты это почти тоже самое, что мы подаем на вход декодеру, но по сути это смещенная вправо последовательность. Мы обрубаем наши тексты до максимальной длины - 1 (не учитывая bos и eos теги), потому что в последовательности 2 и 3 мы добавим по одному тегу (bos и eos). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_sequence = 'мама мыла раму'.split()\n",
    "decoder_sequence = 'bos mom was washing the frame'.split()\n",
    "target_sequence = 'mom was washing the frame eos'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoder_sequence) == len(target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "По слову \"bos\" предсказываем слово \"mom\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"bos\" при условии того, что в памяти есть [] предсказываем слово \"mom\"\n",
      "\n",
      "По слову \"mom\" предсказываем слово \"was\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"mom\" при условии того, что в памяти есть ['bos'] предсказываем слово \"was\"\n",
      "\n",
      "По слову \"was\" предсказываем слово \"washing\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"was\" при условии того, что в памяти есть ['bos', 'mom'] предсказываем слово \"washing\"\n",
      "\n",
      "По слову \"washing\" предсказываем слово \"the\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"washing\" при условии того, что в памяти есть ['bos', 'mom', 'was'] предсказываем слово \"the\"\n",
      "\n",
      "По слову \"the\" предсказываем слово \"frame\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"the\" при условии того, что в памяти есть ['bos', 'mom', 'was', 'washing'] предсказываем слово \"frame\"\n",
      "\n",
      "По слову \"frame\" предсказываем слово \"eos\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"frame\" при условии того, что в памяти есть ['bos', 'mom', 'was', 'washing', 'the'] предсказываем слово \"eos\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(decoder_sequence)):\n",
    "    print(f'По слову \"{decoder_sequence[n]}\" предсказываем слово \"{target_sequence[n]}\"')\n",
    "    print('Или с точки зрения rnn')\n",
    "    print(f'По слову \"{decoder_sequence[n]}\" при условии того, что в памяти есть {decoder_sequence[:n]} предсказываем слово \"{target_sequence[n]}\"')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошо было бы переписать loader с такими последовательностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(torch.utils.data.Dataset):\n",
    "    def __init__(self, context_list, questions_list, context_len, questions_len, pad_index, eos_index):\n",
    "        self.context_list = context_list\n",
    "        self.questions_list = questions_list\n",
    "        \n",
    "        self.context_len = context_len\n",
    "        self.questions_len = questions_len\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.eos_index = eos_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.context_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        encoder_sequence = self.context_list[index][:self.context_len]\n",
    "        decoder_sequence = self.questions_list[index][:self.questions_len]\n",
    "        \n",
    "        target_sequence = decoder_sequence[:] + [self.eos_index]\n",
    "        decoder_sequence = decoder_sequence[:]\n",
    "        \n",
    "        encoder_pads = [self.pad_index] * (self.context_len - len(encoder_sequence))\n",
    "        decoder_pads = [self.pad_index] * (self.questions_len - len(decoder_sequence))\n",
    "        target_pads = [self.pad_index] * (self.questions_len - len(decoder_sequence))\n",
    "        \n",
    "        encoder_sequence = torch.tensor(encoder_sequence + encoder_pads).long()\n",
    "        decoder_sequence = torch.tensor(decoder_sequence + decoder_pads).long()\n",
    "        target_sequence = torch.tensor(target_sequence + target_pads).long()\n",
    "        \n",
    "        return encoder_sequence, decoder_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "context_len = 80\n",
    "quest_len = 20\n",
    "\n",
    "pad_index = 0\n",
    "eos_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start_index = int(len(tokenized_ctx) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77949, 8661)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = WordData(context_list=tokenized_ctx[:-validation_start_index],\n",
    "                         questions_list = tokenized_quest[:-validation_start_index],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "validation_dataset = WordData(context_list=tokenized_ctx[-validation_start_index:],\n",
    "                              questions_list = tokenized_quest[-validation_start_index:],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for encoder_sequence, decoder_sequence, target_sequence in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 21])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSequence2Sequence(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size=16000, embedding_size=256,\n",
    "                 hidden_size=256, num_layers=2, dropout=0.3, padding_idx=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embedding_size, padding_idx)\n",
    "        \n",
    "        self.encoder = torch.nn.GRU(embedding_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.decoder = torch.nn.GRU(hidden_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.head = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, encoder_sequence, decoder_sequence):\n",
    "        \n",
    "        encoder_embed = self.embedding_layer(encoder_sequence)\n",
    "        decoder_embed = self.embedding_layer(decoder_sequence)\n",
    "        \n",
    "        encoder_hidden, encoder_mem = self.encoder(encoder_embed)\n",
    "        \n",
    "        decoder_embed = torch.cat([encoder_hidden[:, -1, :].unsqueeze(1), decoder_embed], dim=1)\n",
    "        \n",
    "        decoder_hidden, _ = self.decoder(decoder_embed, encoder_mem)\n",
    "        \n",
    "        prediction = self.head(decoder_hidden)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer):\n",
    "    \n",
    "   # Put the model in training mode!\n",
    "    model.train()\n",
    "\n",
    "    losses_list = []\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(iterator, total=len(iterator))\n",
    "    \n",
    "    for encoder_sequence, decoder_sequence, target_sequence in progress_bar:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        encoder_sequence = encoder_sequence.to(device)\n",
    "        decoder_sequence = decoder_sequence.to(device)\n",
    "        target_sequence = target_sequence.to(device)\n",
    "\n",
    "        output = model(encoder_sequence, decoder_sequence)\n",
    "\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        target_sequence = target_sequence.view(-1)\n",
    "        \n",
    "        loss = criterion(output, target_sequence)\n",
    "        losses_list.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix(train_loss = np.mean(losses_list[-500:]))\n",
    "        \n",
    "    return losses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleSequence2Sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSequence2Sequence(\n",
       "  (embedding_layer): Embedding(16000, 256, padding_idx=0)\n",
       "  (encoder): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (decoder): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (head): Linear(in_features=256, out_features=16000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c91f38a56f64f30af15a306987b49c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1218.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.684006690979004,\n",
       " 9.638586044311523,\n",
       " 9.589485168457031,\n",
       " 9.55286693572998,\n",
       " 9.46457576751709,\n",
       " 9.412469863891602,\n",
       " 9.349130630493164,\n",
       " 9.154150009155273,\n",
       " 8.853353500366211,\n",
       " 8.244040489196777,\n",
       " 7.932986736297607,\n",
       " 7.649801731109619,\n",
       " 7.49804162979126,\n",
       " 7.196301460266113,\n",
       " 7.379762172698975,\n",
       " 7.186154365539551,\n",
       " 7.146768569946289,\n",
       " 7.1253981590271,\n",
       " 6.963066577911377,\n",
       " 6.713465690612793,\n",
       " 7.028617858886719,\n",
       " 7.389944076538086,\n",
       " 7.379380702972412,\n",
       " 7.3320488929748535,\n",
       " 7.176438331604004,\n",
       " 7.242167949676514,\n",
       " 7.124611854553223,\n",
       " 7.033801078796387,\n",
       " 7.559235095977783,\n",
       " 7.161292552947998,\n",
       " 7.443063735961914,\n",
       " 7.430235862731934,\n",
       " 7.198633193969727,\n",
       " 6.988953113555908,\n",
       " 6.809603214263916,\n",
       " 6.930432319641113,\n",
       " 6.778525352478027,\n",
       " 6.903459072113037,\n",
       " 6.818629264831543,\n",
       " 6.776122093200684,\n",
       " 6.7583441734313965,\n",
       " 6.652819633483887,\n",
       " 6.468145847320557,\n",
       " 6.512057781219482,\n",
       " 6.551897048950195,\n",
       " 6.385016441345215,\n",
       " 6.273505210876465,\n",
       " 6.655798435211182,\n",
       " 6.9085564613342285,\n",
       " 6.989818096160889,\n",
       " 6.9540276527404785,\n",
       " 7.011001110076904,\n",
       " 7.091435432434082,\n",
       " 6.659182071685791,\n",
       " 6.772829055786133,\n",
       " 6.4435133934021,\n",
       " 6.3631591796875,\n",
       " 6.584589004516602,\n",
       " 6.717496395111084,\n",
       " 6.748371601104736,\n",
       " 6.589004993438721,\n",
       " 6.726532459259033,\n",
       " 7.061770439147949,\n",
       " 7.0176777839660645,\n",
       " 7.025257587432861,\n",
       " 6.9678730964660645,\n",
       " 6.84796142578125,\n",
       " 6.681672096252441,\n",
       " 6.859434604644775,\n",
       " 6.815722465515137,\n",
       " 6.928966999053955,\n",
       " 6.909601211547852,\n",
       " 7.053597927093506,\n",
       " 6.743716716766357,\n",
       " 6.669402122497559,\n",
       " 7.007864475250244,\n",
       " 6.829049110412598,\n",
       " 6.742977619171143,\n",
       " 7.094935417175293,\n",
       " 6.903703689575195,\n",
       " 6.964870452880859,\n",
       " 6.8200225830078125,\n",
       " 7.02554988861084,\n",
       " 6.893734455108643,\n",
       " 6.707180500030518,\n",
       " 6.7638163566589355,\n",
       " 6.4452338218688965,\n",
       " 6.408452987670898,\n",
       " 6.403356075286865,\n",
       " 6.368664741516113,\n",
       " 6.293531894683838,\n",
       " 6.436612606048584,\n",
       " 6.2034220695495605,\n",
       " 5.948835849761963,\n",
       " 6.124417781829834,\n",
       " 6.235978603363037,\n",
       " 6.4416351318359375,\n",
       " 7.100815296173096,\n",
       " 6.867760181427002,\n",
       " 7.118290901184082,\n",
       " 6.8410139083862305,\n",
       " 6.9796013832092285,\n",
       " 6.635003566741943,\n",
       " 6.387816905975342,\n",
       " 6.296197414398193,\n",
       " 6.362650394439697,\n",
       " 6.161961078643799,\n",
       " 6.3332133293151855,\n",
       " 6.447697162628174,\n",
       " 6.528130531311035,\n",
       " 6.4772233963012695,\n",
       " 6.801029682159424,\n",
       " 6.7016987800598145,\n",
       " 6.563915252685547,\n",
       " 6.397361755371094,\n",
       " 6.5246381759643555,\n",
       " 6.485711574554443,\n",
       " 7.2230448722839355,\n",
       " 6.877186298370361,\n",
       " 6.857626438140869,\n",
       " 6.509167194366455,\n",
       " 6.516636848449707,\n",
       " 6.41546106338501,\n",
       " 6.168918609619141,\n",
       " 6.2708611488342285,\n",
       " 6.558988094329834,\n",
       " 6.268312931060791,\n",
       " 6.501448631286621,\n",
       " 6.825458526611328,\n",
       " 6.725308418273926,\n",
       " 6.3885979652404785,\n",
       " 6.3861517906188965,\n",
       " 6.3876142501831055,\n",
       " 6.295795440673828,\n",
       " 6.354072093963623,\n",
       " 6.312207221984863,\n",
       " 6.0343828201293945,\n",
       " 5.9120564460754395,\n",
       " 6.187921524047852,\n",
       " 6.74205207824707,\n",
       " 6.767609596252441,\n",
       " 6.795903205871582,\n",
       " 6.299990177154541,\n",
       " 6.40452241897583,\n",
       " 6.644554138183594,\n",
       " 6.400304794311523,\n",
       " 6.29090690612793,\n",
       " 6.5420942306518555,\n",
       " 6.566911697387695,\n",
       " 6.41587495803833,\n",
       " 6.611576080322266,\n",
       " 6.433326244354248,\n",
       " 6.056302547454834,\n",
       " 5.869802474975586,\n",
       " 6.0126447677612305,\n",
       " 6.121801853179932,\n",
       " 6.358789443969727,\n",
       " 6.165172100067139,\n",
       " 6.645251750946045,\n",
       " 6.587090969085693,\n",
       " 6.06886625289917,\n",
       " 6.444398880004883,\n",
       " 6.3528337478637695,\n",
       " 6.260473251342773,\n",
       " 5.999636650085449,\n",
       " 6.1542205810546875,\n",
       " 6.348584175109863,\n",
       " 6.222175598144531,\n",
       " 6.364859580993652,\n",
       " 6.536246299743652,\n",
       " 6.410103797912598,\n",
       " 6.624934673309326,\n",
       " 6.324007511138916,\n",
       " 6.154109001159668,\n",
       " 6.429057598114014,\n",
       " 6.431014060974121,\n",
       " 6.413457870483398,\n",
       " 6.203553199768066,\n",
       " 6.157193660736084,\n",
       " 5.963679790496826,\n",
       " 6.134832382202148,\n",
       " 6.1022725105285645,\n",
       " 7.41990852355957,\n",
       " 6.594980239868164,\n",
       " 6.12140417098999,\n",
       " 6.2537126541137695,\n",
       " 6.581213474273682,\n",
       " 6.659289360046387,\n",
       " 6.251888751983643,\n",
       " 6.552830219268799,\n",
       " 5.751494884490967,\n",
       " 6.343571662902832,\n",
       " 5.732002258300781,\n",
       " 6.2542405128479,\n",
       " 6.367313861846924,\n",
       " 6.077808380126953,\n",
       " 6.049365043640137,\n",
       " 5.984707832336426,\n",
       " 5.763148784637451,\n",
       " 5.779360771179199,\n",
       " 5.804863929748535,\n",
       " 5.959212779998779,\n",
       " 5.694126129150391,\n",
       " 6.081167697906494,\n",
       " 5.623565196990967,\n",
       " 5.760862827301025,\n",
       " 6.320235729217529,\n",
       " 6.519687175750732,\n",
       " 6.45946741104126,\n",
       " 6.108872890472412,\n",
       " 6.315029621124268,\n",
       " 5.834834098815918,\n",
       " 6.221161365509033,\n",
       " 6.6073174476623535,\n",
       " 7.016645908355713,\n",
       " 6.584872722625732,\n",
       " 6.71151065826416,\n",
       " 6.671592712402344,\n",
       " 6.977475643157959,\n",
       " 6.956305980682373,\n",
       " 6.557690143585205,\n",
       " 5.901614665985107,\n",
       " 6.25549840927124,\n",
       " 6.379029273986816,\n",
       " 5.509880542755127,\n",
       " 5.903067588806152,\n",
       " 5.783560752868652,\n",
       " 5.603243350982666,\n",
       " 5.905566692352295,\n",
       " 6.001814365386963,\n",
       " 5.70210075378418,\n",
       " 5.854379177093506,\n",
       " 5.988988876342773,\n",
       " 6.06815767288208,\n",
       " 6.161549091339111,\n",
       " 6.142592430114746,\n",
       " 5.950690746307373,\n",
       " 6.375414848327637,\n",
       " 6.010193347930908,\n",
       " 5.433150291442871,\n",
       " 5.285393714904785,\n",
       " 5.665221214294434,\n",
       " 5.891385555267334,\n",
       " 6.00580358505249,\n",
       " 6.769895553588867,\n",
       " 6.750551223754883,\n",
       " 6.288249969482422,\n",
       " 6.575497150421143,\n",
       " 6.447152137756348,\n",
       " 6.330134868621826,\n",
       " 6.220024108886719,\n",
       " 6.750431537628174,\n",
       " 7.092579364776611,\n",
       " 6.460879325866699,\n",
       " 6.999439239501953,\n",
       " 6.044516086578369,\n",
       " 6.4054856300354,\n",
       " 6.299914836883545,\n",
       " 6.16034460067749,\n",
       " 6.157247066497803,\n",
       " 6.296413898468018,\n",
       " 6.405874252319336,\n",
       " 6.69949197769165,\n",
       " 6.836501121520996,\n",
       " 6.8549909591674805,\n",
       " 6.782848358154297,\n",
       " 6.329251766204834,\n",
       " 6.341675281524658,\n",
       " 6.417400360107422,\n",
       " 6.292182445526123,\n",
       " 6.274138450622559,\n",
       " 5.553125858306885,\n",
       " 5.754471778869629,\n",
       " 5.760181427001953,\n",
       " 5.7775959968566895,\n",
       " 6.05043888092041,\n",
       " 6.138133525848389,\n",
       " 6.371241092681885,\n",
       " 6.24780797958374,\n",
       " 6.238552093505859,\n",
       " 6.866004467010498,\n",
       " 6.206846237182617,\n",
       " 6.16657829284668,\n",
       " 6.065430164337158,\n",
       " 6.176708698272705,\n",
       " 6.037036418914795,\n",
       " 5.968647003173828,\n",
       " 6.100488185882568,\n",
       " 6.357100009918213,\n",
       " 6.488430976867676,\n",
       " 6.121243476867676,\n",
       " 5.539572238922119,\n",
       " 5.5522942543029785,\n",
       " 5.479912281036377,\n",
       " 5.981222629547119,\n",
       " 6.45878791809082,\n",
       " 6.885007381439209,\n",
       " 6.631798267364502,\n",
       " 6.371769905090332,\n",
       " 6.405610084533691,\n",
       " 6.3174333572387695,\n",
       " 6.218936443328857,\n",
       " 6.107517242431641,\n",
       " 5.917259216308594,\n",
       " 6.129130840301514,\n",
       " 5.904046058654785,\n",
       " 5.870478630065918,\n",
       " 6.11631965637207,\n",
       " 5.719316482543945,\n",
       " 5.716869354248047,\n",
       " 6.32760763168335,\n",
       " 6.795114517211914,\n",
       " 7.084722995758057,\n",
       " 6.422192096710205,\n",
       " 6.765745162963867,\n",
       " 6.611746788024902,\n",
       " 6.3968424797058105,\n",
       " 6.388915538787842,\n",
       " 6.07928466796875,\n",
       " 5.954803466796875,\n",
       " 6.11985445022583,\n",
       " 6.155570983886719,\n",
       " 5.7480926513671875,\n",
       " 5.147715091705322,\n",
       " 5.53204345703125,\n",
       " 5.692831039428711,\n",
       " 5.72137975692749,\n",
       " 5.895884990692139,\n",
       " 5.771190166473389,\n",
       " 5.811244487762451,\n",
       " 6.175239562988281,\n",
       " 6.210813999176025,\n",
       " 6.344061374664307,\n",
       " 6.228825092315674,\n",
       " 6.2547407150268555,\n",
       " 5.648550987243652,\n",
       " 5.734983921051025,\n",
       " 5.762655258178711,\n",
       " 5.731451511383057,\n",
       " 6.094754695892334,\n",
       " 6.746384143829346,\n",
       " 6.158027172088623,\n",
       " 5.926090240478516,\n",
       " 5.924196243286133,\n",
       " 6.089234828948975,\n",
       " 5.99785041809082,\n",
       " 6.309500694274902,\n",
       " 6.403255939483643,\n",
       " 6.604708671569824,\n",
       " 6.003418445587158,\n",
       " 5.591429233551025,\n",
       " 6.069540500640869,\n",
       " 5.949441909790039,\n",
       " 5.919546604156494,\n",
       " 6.390891075134277,\n",
       " 5.969743728637695,\n",
       " 5.81935977935791,\n",
       " 6.270593643188477,\n",
       " 6.229066371917725,\n",
       " 6.097188472747803,\n",
       " 6.495824337005615,\n",
       " 6.384671688079834,\n",
       " 6.617592811584473,\n",
       " 6.493956565856934,\n",
       " 5.8028459548950195,\n",
       " 5.71006965637207,\n",
       " 6.037986755371094,\n",
       " 5.98746919631958,\n",
       " 6.138375282287598,\n",
       " 6.448585033416748,\n",
       " 6.228899955749512,\n",
       " 5.945779800415039,\n",
       " 5.644293308258057,\n",
       " 5.423035144805908,\n",
       " 5.860753536224365,\n",
       " 5.79545259475708,\n",
       " 6.16893196105957,\n",
       " 5.811366558074951,\n",
       " 6.436286926269531,\n",
       " 6.121460437774658,\n",
       " 5.397889137268066,\n",
       " 5.7041473388671875,\n",
       " 6.181756019592285,\n",
       " 6.490777015686035,\n",
       " 6.310898780822754,\n",
       " 6.206945419311523,\n",
       " 6.333748817443848,\n",
       " 6.135546684265137,\n",
       " 5.65189790725708,\n",
       " 5.838842391967773,\n",
       " 5.7543840408325195,\n",
       " 5.802347660064697,\n",
       " 5.704273700714111,\n",
       " 5.745792865753174,\n",
       " 6.312020778656006,\n",
       " 5.727407932281494,\n",
       " 5.743121147155762,\n",
       " 5.720546245574951,\n",
       " 6.331206798553467,\n",
       " 6.313117027282715,\n",
       " 6.308145999908447,\n",
       " 6.161696434020996,\n",
       " 5.913649559020996,\n",
       " 5.827213764190674,\n",
       " 6.3232855796813965,\n",
       " 6.197020053863525,\n",
       " 6.4800310134887695,\n",
       " 5.890992641448975,\n",
       " 5.529796600341797,\n",
       " 5.712162017822266,\n",
       " 5.74058723449707,\n",
       " 5.674577236175537,\n",
       " 5.720155715942383,\n",
       " 5.911036491394043,\n",
       " 6.018241882324219,\n",
       " 5.705490589141846,\n",
       " 6.00821590423584,\n",
       " 5.626483917236328,\n",
       " 5.974088191986084,\n",
       " 6.390932559967041,\n",
       " 6.352062702178955,\n",
       " 5.967231750488281,\n",
       " 6.0459885597229,\n",
       " 5.966495990753174,\n",
       " 5.8630051612854,\n",
       " 5.804113864898682,\n",
       " 5.953128337860107,\n",
       " 5.926126956939697,\n",
       " 5.779407978057861,\n",
       " 5.9266815185546875,\n",
       " 6.158409595489502,\n",
       " 5.946401119232178,\n",
       " 5.966972827911377,\n",
       " 6.2286787033081055,\n",
       " 5.818994998931885,\n",
       " 5.9684367179870605,\n",
       " 6.059890270233154,\n",
       " 6.104699611663818,\n",
       " 6.140414714813232,\n",
       " 6.257729530334473,\n",
       " 5.7879838943481445,\n",
       " 6.1526288986206055,\n",
       " 6.475500106811523,\n",
       " 6.291604518890381,\n",
       " 5.899345874786377,\n",
       " 5.703257083892822,\n",
       " 6.09225606918335,\n",
       " 5.955638408660889,\n",
       " 6.163727283477783,\n",
       " 6.377671241760254,\n",
       " 6.338810920715332,\n",
       " 6.126068115234375,\n",
       " 5.881684303283691,\n",
       " 6.037318706512451,\n",
       " 6.180133819580078,\n",
       " 5.969733238220215,\n",
       " 6.093023777008057,\n",
       " 5.987121105194092,\n",
       " 5.535109519958496,\n",
       " 5.469437599182129,\n",
       " 5.654778957366943,\n",
       " 5.795221328735352,\n",
       " 5.656551361083984,\n",
       " 5.76050329208374,\n",
       " 6.106388092041016,\n",
       " 5.38175106048584,\n",
       " 5.1576762199401855,\n",
       " 5.677929878234863,\n",
       " 6.006863117218018,\n",
       " 6.110998630523682,\n",
       " 5.98586893081665,\n",
       " 6.001955986022949,\n",
       " 6.509310245513916,\n",
       " 6.740982532501221,\n",
       " 6.702085971832275,\n",
       " 6.8124494552612305,\n",
       " 6.161794662475586,\n",
       " 5.918954849243164,\n",
       " 5.753958225250244,\n",
       " 5.728418827056885,\n",
       " 6.268063545227051,\n",
       " 6.08986759185791,\n",
       " 6.163805961608887,\n",
       " 6.371718406677246,\n",
       " 6.1720709800720215,\n",
       " 6.103677749633789,\n",
       " 5.737184047698975,\n",
       " 6.062014102935791,\n",
       " 5.873152732849121,\n",
       " 5.775160789489746,\n",
       " 5.648830890655518,\n",
       " 5.43956184387207,\n",
       " 6.035484790802002,\n",
       " 5.9287919998168945,\n",
       " 6.190053939819336,\n",
       " 6.030384063720703,\n",
       " 5.839118003845215,\n",
       " 6.135635852813721,\n",
       " 6.173765659332275,\n",
       " 5.871411323547363,\n",
       " 6.231353759765625,\n",
       " 6.2125563621521,\n",
       " 6.044765949249268,\n",
       " 6.453121185302734,\n",
       " 6.5729079246521,\n",
       " 6.546896934509277,\n",
       " 6.450255393981934,\n",
       " 6.408771991729736,\n",
       " 6.240584850311279,\n",
       " 5.978819847106934,\n",
       " 5.886770725250244,\n",
       " 5.807072639465332,\n",
       " 5.808622360229492,\n",
       " 5.611988067626953,\n",
       " 5.701253414154053,\n",
       " 5.981595993041992,\n",
       " 5.566023349761963,\n",
       " 5.739007472991943,\n",
       " 5.799321174621582,\n",
       " 6.29714298248291,\n",
       " 5.742578029632568,\n",
       " 5.8306193351745605,\n",
       " 6.00771427154541,\n",
       " 5.7847580909729,\n",
       " 6.0921101570129395,\n",
       " 6.564225196838379,\n",
       " 6.453164100646973,\n",
       " 6.396722793579102,\n",
       " 6.037856578826904,\n",
       " 6.020537853240967,\n",
       " 5.930142879486084,\n",
       " 5.666703224182129,\n",
       " 5.895906925201416,\n",
       " 5.616905212402344,\n",
       " 5.5366530418396,\n",
       " 6.04610538482666,\n",
       " 5.9612555503845215,\n",
       " 5.864188194274902,\n",
       " 5.525977611541748,\n",
       " 5.634921073913574,\n",
       " 5.797731876373291,\n",
       " 5.757227420806885,\n",
       " 5.811799049377441,\n",
       " 5.644410610198975,\n",
       " 5.7793779373168945,\n",
       " 5.5666823387146,\n",
       " 5.674022197723389,\n",
       " 5.738808631896973,\n",
       " 5.551369667053223,\n",
       " 5.9554338455200195,\n",
       " 6.138588905334473,\n",
       " 6.169945240020752,\n",
       " 6.001186847686768,\n",
       " 5.682190895080566,\n",
       " 5.674455642700195,\n",
       " 5.495912075042725,\n",
       " 5.622400283813477,\n",
       " 5.789958477020264,\n",
       " 5.828280925750732,\n",
       " 5.823394775390625,\n",
       " 6.076649188995361,\n",
       " 6.3611650466918945,\n",
       " 5.915937423706055,\n",
       " 6.078742027282715,\n",
       " 6.197260856628418,\n",
       " 6.217370510101318,\n",
       " 5.905998706817627,\n",
       " 5.83298921585083,\n",
       " 5.9532976150512695,\n",
       " 5.806532382965088,\n",
       " 6.101129055023193,\n",
       " 6.1859002113342285,\n",
       " 6.167631149291992,\n",
       " 6.214065074920654,\n",
       " 6.369582653045654,\n",
       " 6.448381423950195,\n",
       " 5.535778999328613,\n",
       " 5.641380310058594,\n",
       " 5.832773685455322,\n",
       " 5.81679105758667,\n",
       " 6.2569193840026855,\n",
       " 5.8570075035095215,\n",
       " 5.848388671875,\n",
       " 5.833002090454102,\n",
       " 6.015152454376221,\n",
       " 5.98681640625,\n",
       " 6.1599345207214355,\n",
       " 6.09219217300415,\n",
       " 6.058561325073242,\n",
       " 6.252356052398682,\n",
       " 5.858177185058594,\n",
       " 5.689931392669678,\n",
       " 5.779022216796875,\n",
       " 6.022763729095459,\n",
       " 5.7470831871032715,\n",
       " 5.632932662963867,\n",
       " 5.835799694061279,\n",
       " 6.05725622177124,\n",
       " 5.350128173828125,\n",
       " 5.60271692276001,\n",
       " 5.679569721221924,\n",
       " 5.2093024253845215,\n",
       " 5.286480903625488,\n",
       " 5.383752822875977,\n",
       " 5.429833889007568,\n",
       " 5.871516227722168,\n",
       " 6.270657062530518,\n",
       " 6.159022331237793,\n",
       " 6.007931709289551,\n",
       " 6.226235389709473,\n",
       " 6.1104607582092285,\n",
       " 5.820534706115723,\n",
       " 5.435014724731445,\n",
       " 5.542716979980469,\n",
       " 5.3444342613220215,\n",
       " 5.545018672943115,\n",
       " 5.656667709350586,\n",
       " 6.087497234344482,\n",
       " 5.084509372711182,\n",
       " 5.224373817443848,\n",
       " 5.244251251220703,\n",
       " 5.097282409667969,\n",
       " 5.4336419105529785,\n",
       " 5.372424125671387,\n",
       " 5.726862907409668,\n",
       " 5.640063762664795,\n",
       " 5.461240291595459,\n",
       " 5.6483988761901855,\n",
       " 5.97617244720459,\n",
       " 5.873478889465332,\n",
       " 6.173839092254639,\n",
       " 5.538014888763428,\n",
       " 5.278842926025391,\n",
       " 5.036841869354248,\n",
       " 5.914275646209717,\n",
       " 6.095689296722412,\n",
       " 6.233323574066162,\n",
       " 6.419132709503174,\n",
       " 6.413728713989258,\n",
       " 6.080901622772217,\n",
       " 5.7311835289001465,\n",
       " 5.3460564613342285,\n",
       " 6.119466304779053,\n",
       " 6.112117767333984,\n",
       " 6.31732702255249,\n",
       " 5.852272033691406,\n",
       " 5.9304118156433105,\n",
       " 6.046881198883057,\n",
       " 6.128720760345459,\n",
       " 6.001683235168457,\n",
       " 5.817112445831299,\n",
       " 5.21952486038208,\n",
       " 5.338098526000977,\n",
       " 5.502192974090576,\n",
       " 6.365626335144043,\n",
       " 6.381742000579834,\n",
       " 6.559818267822266,\n",
       " 6.320672035217285,\n",
       " 5.764185428619385,\n",
       " 5.324671745300293,\n",
       " 5.553656578063965,\n",
       " 5.257190704345703,\n",
       " 5.428504467010498,\n",
       " 5.577451229095459,\n",
       " 5.486456871032715,\n",
       " 5.539182662963867,\n",
       " 5.625308990478516,\n",
       " 6.089820861816406,\n",
       " 5.9293599128723145,\n",
       " 5.5961384773254395,\n",
       " 5.95136833190918,\n",
       " 5.759456157684326,\n",
       " 6.190009593963623,\n",
       " 6.084194183349609,\n",
       " 5.830020904541016,\n",
       " 5.575787544250488,\n",
       " 5.827404499053955,\n",
       " 5.710171222686768,\n",
       " 5.751890182495117,\n",
       " 5.842548847198486,\n",
       " 5.373864650726318,\n",
       " 5.659163951873779,\n",
       " 5.659667491912842,\n",
       " 5.597514629364014,\n",
       " 5.8569231033325195,\n",
       " 5.7359514236450195,\n",
       " 5.505668640136719,\n",
       " 5.780020236968994,\n",
       " 5.578545093536377,\n",
       " 5.801544666290283,\n",
       " 5.891139507293701,\n",
       " 6.012742519378662,\n",
       " 6.101178169250488,\n",
       " 6.057565212249756,\n",
       " 5.69111442565918,\n",
       " 6.265470504760742,\n",
       " 5.958817005157471,\n",
       " 5.504413604736328,\n",
       " 5.413173198699951,\n",
       " 5.700742721557617,\n",
       " 5.546506881713867,\n",
       " 5.666864395141602,\n",
       " 5.729160785675049,\n",
       " 5.914145469665527,\n",
       " 5.608221530914307,\n",
       " 6.12308406829834,\n",
       " 6.289248943328857,\n",
       " 5.65322732925415,\n",
       " 5.715605735778809,\n",
       " 6.033885478973389,\n",
       " 5.809114933013916,\n",
       " 6.2198662757873535,\n",
       " 6.016972541809082,\n",
       " 5.984106540679932,\n",
       " 5.773445129394531,\n",
       " 5.678263187408447,\n",
       " 6.085299015045166,\n",
       " 5.792860984802246,\n",
       " 5.709126949310303,\n",
       " 5.8206682205200195,\n",
       " 5.737572193145752,\n",
       " 5.846621513366699,\n",
       " 6.233957767486572,\n",
       " 5.982542037963867,\n",
       " 5.687626838684082,\n",
       " 5.300325870513916,\n",
       " 5.608965873718262,\n",
       " 5.594062805175781,\n",
       " 5.2276997566223145,\n",
       " 5.550934791564941,\n",
       " 5.683828830718994,\n",
       " 4.634716987609863,\n",
       " 4.8726654052734375,\n",
       " 5.3771138191223145,\n",
       " 6.1059417724609375,\n",
       " 5.847066879272461,\n",
       " 5.904192924499512,\n",
       " 6.010796070098877,\n",
       " 5.361562728881836,\n",
       " 5.731572151184082,\n",
       " 5.5219011306762695,\n",
       " 5.621881484985352,\n",
       " 5.904799938201904,\n",
       " 5.904775142669678,\n",
       " 6.006826400756836,\n",
       " 5.941551685333252,\n",
       " 5.92584228515625,\n",
       " 5.917721748352051,\n",
       " 5.537272930145264,\n",
       " 5.446735858917236,\n",
       " 5.150545120239258,\n",
       " 5.884831428527832,\n",
       " 5.81918478012085,\n",
       " 5.657456874847412,\n",
       " 5.515912055969238,\n",
       " 5.547613620758057,\n",
       " 5.69769287109375,\n",
       " 5.4342570304870605,\n",
       " 5.885817050933838,\n",
       " 5.9542999267578125,\n",
       " 5.823111534118652,\n",
       " 5.47580099105835,\n",
       " 5.846216201782227,\n",
       " 6.036799430847168,\n",
       " 6.294980525970459,\n",
       " 5.714059829711914,\n",
       " 5.573152542114258,\n",
       " 5.525746822357178,\n",
       " 5.721893310546875,\n",
       " 5.274540424346924,\n",
       " 5.530002117156982,\n",
       " 5.356721878051758,\n",
       " 6.3092522621154785,\n",
       " 6.150588035583496,\n",
       " 5.811565399169922,\n",
       " 6.113173484802246,\n",
       " 6.410102367401123,\n",
       " 6.250699043273926,\n",
       " 5.629061698913574,\n",
       " 5.657431125640869,\n",
       " 5.92704439163208,\n",
       " 5.53151273727417,\n",
       " 5.075132369995117,\n",
       " 5.278907299041748,\n",
       " 5.887906074523926,\n",
       " 5.991440773010254,\n",
       " 5.810700416564941,\n",
       " 6.0619611740112305,\n",
       " 5.875517845153809,\n",
       " 5.784282684326172,\n",
       " 6.1711745262146,\n",
       " 5.82900857925415,\n",
       " 6.045625686645508,\n",
       " 5.227111339569092,\n",
       " 5.678154468536377,\n",
       " 5.6956048011779785,\n",
       " 5.639692783355713,\n",
       " 5.555306434631348,\n",
       " 6.087757110595703,\n",
       " 6.155862331390381,\n",
       " 5.789096355438232,\n",
       " 5.499902248382568,\n",
       " 5.335643291473389,\n",
       " 5.421708583831787,\n",
       " 5.450798034667969,\n",
       " 5.472452640533447,\n",
       " 5.050239086151123,\n",
       " 5.5612664222717285,\n",
       " 5.446001052856445,\n",
       " 5.615240097045898,\n",
       " 5.669020652770996,\n",
       " 5.9773664474487305,\n",
       " 6.016831398010254,\n",
       " 5.722053527832031,\n",
       " 5.271261692047119,\n",
       " 5.237308502197266,\n",
       " 5.241942882537842,\n",
       " 5.023151874542236,\n",
       " 5.614309787750244,\n",
       " 5.433626174926758,\n",
       " 5.707981586456299,\n",
       " 5.71414852142334,\n",
       " 5.648200988769531,\n",
       " 6.031473159790039,\n",
       " 5.662656307220459,\n",
       " 5.6497721672058105,\n",
       " 6.054982662200928,\n",
       " 5.967550277709961,\n",
       " 5.841970443725586,\n",
       " 5.69292688369751,\n",
       " 5.7076568603515625,\n",
       " 6.248878479003906,\n",
       " 5.9281487464904785,\n",
       " 5.752933979034424,\n",
       " 5.579864025115967,\n",
       " 5.498115062713623,\n",
       " 5.665334224700928,\n",
       " 5.598026275634766,\n",
       " 5.674818515777588,\n",
       " 5.630655765533447,\n",
       " 5.370909214019775,\n",
       " 5.8496599197387695,\n",
       " 5.724750518798828,\n",
       " 6.169252395629883,\n",
       " 5.462581634521484,\n",
       " 5.718411445617676,\n",
       " 5.738805294036865,\n",
       " 5.898871421813965,\n",
       " 5.908660411834717,\n",
       " 5.889747142791748,\n",
       " 6.361059665679932,\n",
       " 6.509551525115967,\n",
       " 5.495668411254883,\n",
       " 5.4597978591918945,\n",
       " 5.790482997894287,\n",
       " 5.806459426879883,\n",
       " 4.902929306030273,\n",
       " 5.058623313903809,\n",
       " 5.329823017120361,\n",
       " 5.041823863983154,\n",
       " 5.2053704261779785,\n",
       " 5.568103313446045,\n",
       " 5.592219352722168,\n",
       " 5.706974029541016,\n",
       " 5.89504861831665,\n",
       " 5.917520046234131,\n",
       " 5.968214988708496,\n",
       " 5.461689472198486,\n",
       " 4.946934223175049,\n",
       " 5.35833740234375,\n",
       " 5.343569755554199,\n",
       " 5.514212608337402,\n",
       " 5.532928466796875,\n",
       " 5.765636444091797,\n",
       " 5.661426067352295,\n",
       " 5.807560920715332,\n",
       " 5.78463888168335,\n",
       " 6.148896217346191,\n",
       " 5.593991756439209,\n",
       " 5.006609916687012,\n",
       " 5.107244968414307,\n",
       " 5.453914165496826,\n",
       " 5.318090915679932,\n",
       " 5.299010276794434,\n",
       " 5.195037364959717,\n",
       " 5.105524063110352,\n",
       " 5.417897701263428,\n",
       " 5.219447135925293,\n",
       " 5.51424503326416,\n",
       " 5.368630409240723,\n",
       " 4.869580268859863,\n",
       " 5.416621208190918,\n",
       " 5.239864826202393,\n",
       " 5.506669998168945,\n",
       " 5.545468330383301,\n",
       " 5.210369110107422,\n",
       " 5.292418003082275,\n",
       " 5.724879264831543,\n",
       " 5.487349987030029,\n",
       " 5.624295711517334,\n",
       " 6.225801467895508,\n",
       " 5.854896545410156,\n",
       " 6.0768632888793945,\n",
       " 5.613162040710449,\n",
       " 5.253591537475586,\n",
       " 5.5018439292907715,\n",
       " 5.993149757385254,\n",
       " 5.929715156555176,\n",
       " 5.799747943878174,\n",
       " 6.040950775146484,\n",
       " 6.390712738037109,\n",
       " 6.407079219818115,\n",
       " 6.201730728149414,\n",
       " 6.019803047180176,\n",
       " 6.052127838134766,\n",
       " 5.84831428527832,\n",
       " 5.935521125793457,\n",
       " 5.147627353668213,\n",
       " 5.695265769958496,\n",
       " 5.717743873596191,\n",
       " 6.08027982711792,\n",
       " 6.148746013641357,\n",
       " 6.0751953125,\n",
       " 5.266042232513428,\n",
       " 4.971913814544678,\n",
       " 4.94617223739624,\n",
       " 5.235315799713135,\n",
       " 6.039374351501465,\n",
       " 5.804322719573975,\n",
       " 5.685256481170654,\n",
       " 5.2080864906311035,\n",
       " 5.6605000495910645,\n",
       " 5.72186803817749,\n",
       " 5.755712032318115,\n",
       " 5.366183757781982,\n",
       " 4.9886322021484375,\n",
       " 4.9352264404296875,\n",
       " 4.961699962615967,\n",
       " 5.47762393951416,\n",
       " 5.478272914886475,\n",
       " 5.612199306488037,\n",
       " 5.965153694152832,\n",
       " 5.586580276489258,\n",
       " 6.042602062225342,\n",
       " 6.236658573150635,\n",
       " 6.381150245666504,\n",
       " 5.769191741943359,\n",
       " 5.816943645477295,\n",
       " 5.6899495124816895,\n",
       " 5.8891143798828125,\n",
       " 5.8773722648620605,\n",
       " 5.769717216491699,\n",
       " 5.932504653930664,\n",
       " 5.60333251953125,\n",
       " 5.692584991455078,\n",
       " 5.7762837409973145,\n",
       " 6.090698719024658,\n",
       " 5.743735313415527,\n",
       " 5.931451797485352,\n",
       " 5.426071643829346,\n",
       " 5.21273136138916,\n",
       " 5.0203633308410645,\n",
       " 5.312769412994385,\n",
       " 5.529113292694092,\n",
       " 5.021691799163818,\n",
       " 5.287566184997559,\n",
       " 5.693204402923584,\n",
       " 5.467978477478027,\n",
       " 5.955527305603027,\n",
       " 6.05549955368042,\n",
       " 5.612675666809082,\n",
       " 5.4790754318237305,\n",
       " 5.2833709716796875,\n",
       " 5.408154487609863,\n",
       " 5.659295558929443,\n",
       " 5.770668029785156,\n",
       " 5.738951206207275,\n",
       " 5.543086528778076,\n",
       " 5.834118366241455,\n",
       " 5.433260440826416,\n",
       " 5.868232727050781,\n",
       " 5.913636207580566,\n",
       " 6.049297332763672,\n",
       " 5.973853588104248,\n",
       " 6.088173866271973,\n",
       " 5.732633113861084,\n",
       " 5.444024562835693,\n",
       " 5.803070068359375,\n",
       " 5.300971031188965,\n",
       " 5.836134910583496,\n",
       " 5.879266738891602,\n",
       " 5.94577693939209,\n",
       " 5.795804977416992,\n",
       " 6.125854015350342,\n",
       " 5.968344688415527,\n",
       " 4.99639368057251,\n",
       " 6.119880676269531,\n",
       " 6.111756324768066,\n",
       " 5.865721702575684,\n",
       " 5.663956165313721,\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = tokenizer.vocab().index(\"<PAD>\")\n",
    "eos_idx = tokenizer.vocab().index(\"<EOS>\")\n",
    "sos_idx = tokenizer.vocab().index(\"<BOS>\")\n",
    "# Size of embedding_dim should match the dim of pre-trained word embeddings!\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "vocab_size = len(tokenizer.vocab())\n",
    "model = My_seq2seq_attention(embedding_dim,\n",
    "                 hidden_dim, \n",
    "                 vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=1.0e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "train_losses = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, epoch)\n",
    "#     print (train_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    if min(train_losses) == train_loss and len(train_losses) > 1:\n",
    "        torch.save(model.state_dict, \"best_seq2seq_attention\")\n",
    "        torch.save(optimizer.state_dict, \"best_Adam_state_dict_attention\")\n",
    "    \n",
    "    torch.save(model.state_dict, \"last_seq2seq_attention\")\n",
    "    torch.save(optimizer.state_dict, \"Adam_state_dict_attention\")\n",
    "    \n",
    "    #early stopping\n",
    "    validation_losses = []\n",
    "    test_loss = evaluate(model, validation_loader)\n",
    "    validation_losses.append(test_loss)\n",
    "    \n",
    "    if len(validation_losses) > 1 and validation_losses[epoch] > validation_losses[epoch-1]:\n",
    "        print(\"stop\")\n",
    "        break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN_inside_class(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, embedding):#input_size\n",
    "        super(EncoderRNN_inside_class, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True: \n",
    "            print(\"====ENCODING_FORWARD====\")\n",
    "            print(\"input.shape\", input.shape)\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        if debug == True: \n",
    "            print(\"embedded/output.shape\",embedded.shape,\"hidden.shape\", hidden.shape  )\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# исправленный\n",
    "# необязательно задавать хайден, читай доку\n",
    "class EncoderRNN_inside_class(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, embedding):#input_size\n",
    "        super(EncoderRNN_inside_class, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "\n",
    "        embedded = self.embedding(sequence)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder_inside_class(nn.Module):\n",
    "    def __init__(self, embedding, embedding_size,\n",
    "                 hidden_size, output_size):\n",
    "        super(AttentionDecoder_inside_class, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, 80)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, debug = False):\n",
    "        embedded = self.embedding(input)\n",
    "        if debug == True:\n",
    "            print(\"embedded[0].shape {}, hidden.shape {}\".format(embedded[0].shape, hidden.shape))\n",
    "        attn_weights = torch.softmax(self.attn(torch.cat((embedded[0], hidden), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        if debug == True: \n",
    "            print(\"output.shape {}, hidden.shape {}\".format(output.shape,hidden.shape ))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        hidden = hidden.squeeze(1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_seq2seq_attention(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx):\n",
    "        super(My_seq2seq_attention, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Encoder network\n",
    "        self.encoder = EncoderRNN_inside_class(hidden_size, \n",
    "                               embedding_size, \n",
    "                               self.embedding)\n",
    "        \n",
    "        # Decoder network        \n",
    "        self.decoder = AttentionDecoder_inside_class(self.embedding,\n",
    "                               embedding_size,\n",
    "                              hidden_size,\n",
    "                              vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, input_sequence, output_sequence, debug = False):\n",
    "        \n",
    "\n",
    "        encoder_output, encoder_hidden = self.encoder(input_sequence)\n",
    "        batch_size = len(input_sequence)\n",
    "        outputs = torch.zeros(batch_size, 20, self.vocab_size).to(self.device)\n",
    "        \n",
    "        for batch_element_index in range(batch_size):\n",
    "            \n",
    "            target_tensor = output_sequence[batch_element_index, :]\n",
    "                \n",
    "            decoder_input = torch.tensor([[self.sos_idx]], device=device)\n",
    "\n",
    "            decoder_hidden = encoder_hidden[:,batch_element_index,:]\n",
    "\n",
    "            encoder_output_in_current_index = encoder_output[batch_element_index,:,:]\n",
    "\n",
    "            for di in range(20):\n",
    "                \n",
    "                decoder_output, decoder_hidden = self.decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_output_in_current_index)\n",
    "                \n",
    "                outputs[batch_element_index,di,:] = decoder_output\n",
    "                \n",
    "                if target_tensor[di] != self.pad_idx:\n",
    "                    decoder_input = torch.tensor([target_tensor[di]], device=device).unsqueeze(1)\n",
    "                else:\n",
    "                    break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer,epoch_number, debug = False):\n",
    "   # Put the model in training mode!\n",
    "    model.train()\n",
    "\n",
    "    losses_list = []\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(iterator), total=len(iterator), desc='Epoch {}'.format(epoch_number + 1))\n",
    "    for idx, batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_seq = torch.tensor(batch[0]).to(device)\n",
    "        target_tokens = torch.tensor(batch[1]).to(device)\n",
    "\n",
    "        output = model(input_seq, target_tokens)\n",
    "\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        target_tokens = target_tokens.view(-1)\n",
    "        loss = criterion(output, target_tokens)\n",
    "        losses_list.append(float(loss))\n",
    "        progress_bar.set_postfix(train_loss = np.mean(losses_list[-100:]))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "#         break\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (model, iterator):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(iterator), total=len(iterator))\n",
    "    for idx, batch in progress_bar:\n",
    "        with torch.no_grad():\n",
    "            input_seq = torch.tensor(batch[0]).to(device)\n",
    "            target_tokens = torch.tensor(batch[1]).to(device)\n",
    "            output = model(input_seq, target_tokens)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            target_tokens = target_tokens.view(-1)\n",
    "            loss = criterion(output, target_tokens)\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(train_loss = np.mean(losses_list[-100:]))\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for idx, batch in tqdm(enumerate(validation_loader), total=len(validation_loader)):\n",
    "        print(\"0\", batch[0])\n",
    "        print(\"1\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = tokenizer.vocab().index(\"<PAD>\")\n",
    "eos_idx = tokenizer.vocab().index(\"<EOS>\")\n",
    "sos_idx = tokenizer.vocab().index(\"<BOS>\")\n",
    "# Size of embedding_dim should match the dim of pre-trained word embeddings!\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "vocab_size = len(tokenizer.vocab())\n",
    "model = My_seq2seq_attention(embedding_dim,\n",
    "                 hidden_dim, \n",
    "                 vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=1.0e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss = train(model, train_loader, criterion, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = train_loader\n",
    "epoch_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94161795ab2b4a81aa35f94fc08bf326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=1286.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "losses_list = []\n",
    "epoch_loss = 0\n",
    "progress_bar = tqdm(enumerate(iterator), total=len(iterator), desc='Epoch {}'.format(epoch_number + 1))\n",
    "for idx, batch in progress_bar:\n",
    "    optimizer.zero_grad()\n",
    "    input_seq = torch.tensor(batch[0]).to(device)\n",
    "    target_tokens = torch.tensor(batch[1]).to(device)\n",
    "\n",
    "    break\n",
    "    \n",
    "    output = model(input_seq, target_tokens)\n",
    "    output = output.view(-1, output.size(-1))\n",
    "    target_tokens = target_tokens.view(-1)\n",
    "    \n",
    "    loss = criterion(output, target_tokens)\n",
    "    losses_list.append(loss.item())\n",
    "    progress_bar.set_postfix(train_loss = np.mean(losses_list[-100:]))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in iterator:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  3297,  1394,  ...,  8292,  1300,  9914],\n",
       "        [    2,  3297,  1394,  ...,  8292,  1300,  9914],\n",
       "        [    2,  2570,  4376,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,  3917, 10861,  ...,  3711,  1487,  9139],\n",
       "        [    2,  3308,  1585,  ...,     0,     0,     0],\n",
       "        [    2,  2733,  1462,  ...,  2102,  2904,  1287]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2, 1800, 1503,  ...,    0,    0,    0],\n",
       "        [   2, 1411, 2461,  ...,    0,    0,    0],\n",
       "        [   2, 1800, 1503,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   2, 1615, 2459,  ...,    0,    0,    0],\n",
       "        [   2, 1411, 4162,  ...,    0,    0,    0],\n",
       "        [   2, 1723, 5054,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_sequence, output_sequence in iterator:\n",
    "    input_sequence = input_sequence.to(device)\n",
    "    output_sequence = output_sequence.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 80]), torch.Size([64, 20]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence.shape, output_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output, encoder_hidden = model.encoder(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 300])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSequence2Sequence(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size=16000, embedding_size=256,\n",
    "                 hidden_size=256, num_layers=2, dropout=0.3, padding_idx=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embedding_size, padding_idx)\n",
    "        \n",
    "        self.encoder = torch.nn.GRU(embedding_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.decoder = torch.nn.GRU(hidden_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.head = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, encoder_sequence, decoder_sequence):\n",
    "        \n",
    "        encoder_embed = self.embedding_layer(encoder_sequence)\n",
    "        decoder_embed = self.embedding_layer(decoder_sequence)\n",
    "        \n",
    "        encoder_hidden, encoder_mem = self.encoder(encoder_embed)\n",
    "        \n",
    "        decoder_embed = torch.cat([encoder_hidden[:, -1, :].unsqueeze(1), decoder_embed], dim=1)\n",
    "        \n",
    "        decoder_hidden, _ = self.decoder(decoder_embed, encoder_mem)\n",
    "        \n",
    "        prediction = self.head(decoder_hidden)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleSequence2Sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSequence2Sequence(\n",
       "  (embedding_layer): Embedding(16000, 256, padding_idx=0)\n",
       "  (encoder): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (decoder): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (head): Linear(in_features=256, out_features=16000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for encoder_sequence, decoder_sequence in iterator:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(encoder_sequence, decoder_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 21, 16000])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = len(input_sequence)\n",
    "outputs = torch.zeros(batch_size, 20, self.vocab_size).to(self.device)\n",
    "\n",
    "for batch_element_index in range(batch_size):\n",
    "\n",
    "    target_tensor = output_sequence[batch_element_index, :]\n",
    "\n",
    "    decoder_input = torch.tensor([[self.sos_idx]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden[:,batch_element_index,:]\n",
    "\n",
    "    encoder_output_in_current_index = encoder_output[batch_element_index,:,:]\n",
    "\n",
    "    for di in range(20):\n",
    "\n",
    "        decoder_output, decoder_hidden = self.decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output_in_current_index)\n",
    "\n",
    "        outputs[batch_element_index,di,:] = decoder_output\n",
    "\n",
    "        if target_tensor[di] != self.pad_idx:\n",
    "            decoder_input = torch.tensor([target_tensor[di]], device=device).unsqueeze(1)\n",
    "        else:\n",
    "            break\n",
    "return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a46c87609e5423abade8569a2fd691d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=1286.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "pad_idx = tokenizer.vocab().index(\"<PAD>\")\n",
    "eos_idx = tokenizer.vocab().index(\"<EOS>\")\n",
    "sos_idx = tokenizer.vocab().index(\"<BOS>\")\n",
    "# Size of embedding_dim should match the dim of pre-trained word embeddings!\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "vocab_size = len(tokenizer.vocab())\n",
    "model = My_seq2seq_attention(embedding_dim,\n",
    "                 hidden_dim, \n",
    "                 vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=1.0e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "train_losses = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, epoch)\n",
    "#     print (train_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    if min(train_losses) == train_loss and len(train_losses) > 1:\n",
    "        torch.save(model.state_dict, \"best_seq2seq_attention\")\n",
    "        torch.save(optimizer.state_dict, \"best_Adam_state_dict_attention\")\n",
    "    \n",
    "    torch.save(model.state_dict, \"last_seq2seq_attention\")\n",
    "    torch.save(optimizer.state_dict, \"Adam_state_dict_attention\")\n",
    "    \n",
    "    #early stopping\n",
    "    validation_losses = []\n",
    "    test_loss = evaluate(model, validation_loader)\n",
    "    validation_losses.append(test_loss)\n",
    "    \n",
    "    if len(validation_losses) > 1 and validation_losses[epoch] > validation_losses[epoch-1]:\n",
    "        print(\"stop\")\n",
    "        break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
