{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ЗАДАЧИ\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "0) Сделать бпе токенизацию\n",
    "1) сделать один класс seq2seq\n",
    "2) правильно рассчитывать лосс, не учитывая паддинг\n",
    "правильно считать лосс, добавив токен bos (begin of sentence) в ту последовательность, \n",
    "которую подаешь в декодер и токен eos в таргет последовательность\n",
    "3) впилить early stopping  и добавить больше эпох\n",
    "и early stopping нужно впиливать аккуратно: сохранять лучшую модель, но и отдельно на каждой эпохе\n",
    "\n",
    "что-то типа:\n",
    "best_model_state_dict.pth и last_model_state_dict.pth\n",
    "\n",
    "также хорошо бы сохранять еще state_dict оптимизаторов, так как в них хранятся статистики, на основе которых делаются \n",
    "все эти хитрые смещения типа моментума. сохранять стейт дикт оптимизатора нужно, если хочешь потом дообучать модель\n",
    "\n",
    "это делается тоже просто\n",
    "torch.save(seq2seq_optimizer.state_dict(), 'last_optimizer_state_dict.pth')\n",
    "...\n",
    "\n",
    "4) добавить фичи как в статье (позиционное кодирование, кодирование частей речи, кодирование слов, отвечающих на потенциальный вопрос)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6MB)\n",
      "\u001b[K     |████████████████████████████████| 734.6MB 68kB/s s eta 0:00:01     |███████████████████████▏        | 531.5MB 63.7MB/s eta 0:00:04     |██████████████████████████████  | 689.8MB 62.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.3.1\n",
      "Collecting tqdm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/32/5144caf0478b1f26bd9d97f510a47336cf4ac0f96c6bc3b5af20d4173920/tqdm-4.40.2-py2.py3-none-any.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 791kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.40.2\n",
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 1.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.13.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1450714 sha256=d442d3def43b67d4b1a64b9e01fe9ae1ad49827f908c4e41ca1ad147ddda82d5\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n",
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4MB 1.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 51.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.13.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-0.25.3 pytz-2019.3\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.3)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  wget\n",
      "0 upgraded, 1 newly installed, 0 to remove and 29 not upgraded.\n",
      "Need to get 316 kB of archives.\n",
      "After this operation, 954 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 wget amd64 1.19.4-1ubuntu2.2 [316 kB]\n",
      "Fetched 316 kB in 1s (445 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package wget.\n",
      "(Reading database ... 16326 files and directories currently installed.)\n",
      "Preparing to unpack .../wget_1.19.4-1ubuntu2.2_amd64.deb ...\n",
      "Unpacking wget (1.19.4-1ubuntu2.2) ...\n",
      "Setting up wget (1.19.4-1ubuntu2.2) ...\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install tqdm\n",
    "!pip3 install nltk\n",
    "!pip3 install pandas\n",
    "!pip3 install numpy\n",
    "!pip3 install seaborn\n",
    "!apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            context  \\\n",
       "0           0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1           1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2           2  Managed by her father, Mathew Knowles, the gro...   \n",
       "3           3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4           4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question  \n",
       "0           When did Beyonce start becoming popular?  \n",
       "1  What areas did Beyonce compete in when she was...  \n",
       "2  When did Beyonce leave Destiny's Child and bec...  \n",
       "3      In what city and state did Beyonce  grow up?   \n",
       "4         In which decade did Beyonce become famous?  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ИЗМЕНИТЬ ПУТЬ В ЗАВИСИМОСТИ ОТ ТОГО ГДЕ ЗАПУСКАЕМ КОД\n",
    "try:\n",
    "    df = pd.read_csv(\"ctx_quest.csv\")\n",
    "except:\n",
    "    df = pd.read_csv(\"/Users/lilyakhoang/input/question_generation/ctx_quest.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 86821 entries, 0 to 86820\n",
      "Data columns (total 3 columns):\n",
      "Unnamed: 0    86821 non-null int64\n",
      "context       86610 non-null object\n",
      "question      86821 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 86610 entries, 0 to 86820\n",
      "Data columns (total 3 columns):\n",
      "Unnamed: 0    86610 non-null int64\n",
      "context       86610 non-null object\n",
      "question      86610 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_nonan = df.dropna()\n",
    "df_nonan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# для \"обучения\" bpe модели нам нужно сохранить данные для обучения в отдельный файл\n",
    "# где будут построчно храниться тексты\n",
    "err = 0\n",
    "with open('/Users/lilyakhoang/input/for_bpe_ctx_quest.txt', 'w') as f:\n",
    "    for que in df_nonan.context:\n",
    "        try:\n",
    "            f.write(que + '\\n')\n",
    "        except:\n",
    "            err += 1\n",
    "    for que in df_nonan.question:\n",
    "        try:\n",
    "            f.write(que + '\\n')\n",
    "        except:\n",
    "            err += 1\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtokentome as yttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x1a1ea13cf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучаем\n",
    "vocab_size = 16000\n",
    "model_path = 'bpe.model'\n",
    "\n",
    "yttm.BPE.train(data='/Users/lilyakhoang/input/for_bpe_ctx_quest.txt', vocab_size=vocab_size, model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<BOS>',\n",
       "  '▁Beyoncé',\n",
       "  '▁G',\n",
       "  'is',\n",
       "  'elle',\n",
       "  '▁Know',\n",
       "  'les',\n",
       "  '-C',\n",
       "  'arter',\n",
       "  '▁(/',\n",
       "  'bi',\n",
       "  'ː',\n",
       "  'ˈ',\n",
       "  'j',\n",
       "  'ɒ',\n",
       "  'n',\n",
       "  'se',\n",
       "  'ɪ',\n",
       "  '/',\n",
       "  '▁be',\n",
       "  'e',\n",
       "  '-',\n",
       "  'Y',\n",
       "  'O',\n",
       "  'N',\n",
       "  '-s',\n",
       "  'ay',\n",
       "  ')',\n",
       "  '▁(b',\n",
       "  'orn',\n",
       "  '▁September',\n",
       "  '▁4,',\n",
       "  '▁198',\n",
       "  '1)',\n",
       "  '▁is',\n",
       "  '▁an',\n",
       "  '▁American',\n",
       "  '▁sing',\n",
       "  'er,',\n",
       "  '▁song',\n",
       "  'writ',\n",
       "  'er,',\n",
       "  '▁record',\n",
       "  '▁producer',\n",
       "  '▁and',\n",
       "  '▁act',\n",
       "  'ress.',\n",
       "  '▁B',\n",
       "  'orn',\n",
       "  '▁and',\n",
       "  '▁raised',\n",
       "  '▁in',\n",
       "  '▁Hou',\n",
       "  'ston,',\n",
       "  '▁Texas,',\n",
       "  '▁she',\n",
       "  '▁performed',\n",
       "  '▁in',\n",
       "  '▁various',\n",
       "  '▁singing',\n",
       "  '▁and',\n",
       "  '▁d',\n",
       "  'ancing',\n",
       "  '▁competitions',\n",
       "  '▁as',\n",
       "  '▁a',\n",
       "  '▁child,',\n",
       "  '▁and',\n",
       "  '▁rose',\n",
       "  '▁to',\n",
       "  '▁fame',\n",
       "  '▁in',\n",
       "  '▁the',\n",
       "  '▁late',\n",
       "  '▁1990s',\n",
       "  '▁as',\n",
       "  '▁lead',\n",
       "  '▁singer',\n",
       "  '▁of',\n",
       "  '▁R&B',\n",
       "  '▁girl',\n",
       "  '-g',\n",
       "  'roup',\n",
       "  \"▁Destiny's\",\n",
       "  '▁Child',\n",
       "  '.',\n",
       "  '▁Man',\n",
       "  'aged',\n",
       "  '▁by',\n",
       "  '▁her',\n",
       "  '▁father,',\n",
       "  '▁Mat',\n",
       "  'hew',\n",
       "  '▁Know',\n",
       "  'les,',\n",
       "  '▁the',\n",
       "  '▁group',\n",
       "  '▁became',\n",
       "  '▁one',\n",
       "  '▁of',\n",
       "  '▁the',\n",
       "  \"▁world's\",\n",
       "  '▁best-selling',\n",
       "  '▁girl',\n",
       "  '▁groups',\n",
       "  '▁of',\n",
       "  '▁all',\n",
       "  '▁time.',\n",
       "  '<EOS>'],\n",
       " ['<BOS>',\n",
       "  '▁Beyoncé',\n",
       "  '▁G',\n",
       "  'is',\n",
       "  'elle',\n",
       "  '▁Know',\n",
       "  'les',\n",
       "  '-C',\n",
       "  'arter',\n",
       "  '▁(/',\n",
       "  'bi',\n",
       "  'ː',\n",
       "  'ˈ',\n",
       "  'j',\n",
       "  'ɒ',\n",
       "  'n',\n",
       "  'se',\n",
       "  'ɪ',\n",
       "  '/',\n",
       "  '▁be',\n",
       "  'e',\n",
       "  '-',\n",
       "  'Y',\n",
       "  'O',\n",
       "  'N',\n",
       "  '-s',\n",
       "  'ay',\n",
       "  ')',\n",
       "  '▁(b',\n",
       "  'orn',\n",
       "  '▁September',\n",
       "  '▁4,',\n",
       "  '▁198',\n",
       "  '1)',\n",
       "  '▁is',\n",
       "  '▁an',\n",
       "  '▁American',\n",
       "  '▁sing',\n",
       "  'er,',\n",
       "  '▁song',\n",
       "  'writ',\n",
       "  'er,',\n",
       "  '▁record',\n",
       "  '▁producer',\n",
       "  '▁and',\n",
       "  '▁act',\n",
       "  'ress.',\n",
       "  '▁B',\n",
       "  'orn',\n",
       "  '▁and',\n",
       "  '▁raised',\n",
       "  '▁in',\n",
       "  '▁Hou',\n",
       "  'ston,',\n",
       "  '▁Texas,',\n",
       "  '▁she',\n",
       "  '▁performed',\n",
       "  '▁in',\n",
       "  '▁various',\n",
       "  '▁singing',\n",
       "  '▁and',\n",
       "  '▁d',\n",
       "  'ancing',\n",
       "  '▁competitions',\n",
       "  '▁as',\n",
       "  '▁a',\n",
       "  '▁child,',\n",
       "  '▁and',\n",
       "  '▁rose',\n",
       "  '▁to',\n",
       "  '▁fame',\n",
       "  '▁in',\n",
       "  '▁the',\n",
       "  '▁late',\n",
       "  '▁1990s',\n",
       "  '▁as',\n",
       "  '▁lead',\n",
       "  '▁singer',\n",
       "  '▁of',\n",
       "  '▁R&B',\n",
       "  '▁girl',\n",
       "  '-g',\n",
       "  'roup',\n",
       "  \"▁Destiny's\",\n",
       "  '▁Child',\n",
       "  '.',\n",
       "  '▁Man',\n",
       "  'aged',\n",
       "  '▁by',\n",
       "  '▁her',\n",
       "  '▁father,',\n",
       "  '▁Mat',\n",
       "  'hew',\n",
       "  '▁Know',\n",
       "  'les,',\n",
       "  '▁the',\n",
       "  '▁group',\n",
       "  '▁became',\n",
       "  '▁one',\n",
       "  '▁of',\n",
       "  '▁the',\n",
       "  \"▁world's\",\n",
       "  '▁best-selling',\n",
       "  '▁girl',\n",
       "  '▁groups',\n",
       "  '▁of',\n",
       "  '▁all',\n",
       "  '▁time.',\n",
       "  '<EOS>']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(list(df_nonan.context[:2]), bos=True, eos=True, output_type=yttm.OutputType.SUBWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 339/339 [00:05<00:00, 59.13it/s]\n",
      "100%|██████████| 339/339 [00:01<00:00, 194.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# давайте токенизируем наш датасет\n",
    "# токенизирую батчами, потому что так быстрее\n",
    "# также в начало добавляем токен bos (begin of sentence)\n",
    "\n",
    "tokenized_ctx = []\n",
    "tokenized_quest = []\n",
    "batch_size = 256\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(df_nonan.context) / batch_size))):\n",
    "    \n",
    "    tokenized_ctx.extend(tokenizer.encode(list(df_nonan.context[i_batch*batch_size:(i_batch+1)*batch_size]), bos=True))\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(df_nonan.question) / batch_size))):\n",
    "\n",
    "    tokenized_quest.extend(tokenizer.encode(list(df_nonan.question[i_batch*batch_size:(i_batch+1)*batch_size]), bos = True, eos=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1411, 2461, 1503, 7635, 9910, 1305, 1795, 2402, 1361, 5194, 1761, 36, 3]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_quest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " '<UNK>',\n",
       " '<BOS>',\n",
       " '<EOS>',\n",
       " '▁',\n",
       " 'e',\n",
       " 't',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 'h',\n",
       " 'l']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(torch.utils.data.Dataset):\n",
    "    def __init__(self, context_list, questions_list, context_len, questions_len, pad_index, eos_index):\n",
    "        self.context_list = context_list\n",
    "        self.questions_list = questions_list\n",
    "        \n",
    "        self.context_len = context_len\n",
    "        self.questions_len = questions_len\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.eos_index = eos_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.context_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        context = self.context_list[index][:self.context_len]\n",
    "        pads_ctx = [self.pad_index] * (self.context_len - len(context))\n",
    "#         print(len(pads_ctx))\n",
    "        context = torch.tensor(context + pads_ctx).long()\n",
    "        \n",
    "        question = self.questions_list[index][:self.questions_len]\n",
    "        pads_quest = [self.pad_index] * (self.questions_len - len(question))\n",
    "        question = torch.tensor(question + pads_quest).long()\n",
    "        \n",
    "        return context, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAD>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab()[pad_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "context_len = 80\n",
    "quest_len = 20\n",
    "\n",
    "pad_index = 0\n",
    "eos_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start_index = int(len(tokenized_ctx) * 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82280, 4330)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = WordData(context_list=tokenized_ctx[:-validation_start_index], questions_list = tokenized_quest[:-validation_start_index],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "validation_dataset = WordData(context_list=tokenized_ctx[-validation_start_index:],questions_list = tokenized_quest[-validation_start_index:],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    2,  3297,  1394,  1304, 13539, 12243,  1838,  4502,  4113,  8059,\n",
       "         13895,   100,    90,    65,   197,     9,  1444,   112,    76,  1348,\n",
       "             5,    42,    73,    62,    51,  2250,  1398,    47,  4690,  2117,\n",
       "          2811,  6982,  2156,  8762,  1352,  1306,  1959,  2339,  2087,  3046,\n",
       "          7582,  2087,  2083,  7536,  1312,  1875, 11109,  1365,  2117,  1312,\n",
       "          5267,  1305,  4274, 13461, 15846,  2402,  4829,  1305,  2734, 10861,\n",
       "          1312,  1317,  9041, 12046,  1363,  1285, 12745,  1312,  7309,  1319,\n",
       "         13738,  1305,  1287,  2535,  7915,  1363,  2018,  8292,  1300,  9914]),\n",
       " tensor([   2, 1800, 1503, 7635, 2294, 4596, 2454,   36,    3,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,  3297,  1394,  1304, 13539, 12243,  1838,  4502,  4113,  8059,\n",
      "        13895,   100,    90,    65,   197,     9,  1444,   112,    76,  1348,\n",
      "            5,    42,    73,    62,    51,  2250,  1398,    47,  4690,  2117,\n",
      "         2811,  6982,  2156,  8762,  1352,  1306,  1959,  2339,  2087,  3046,\n",
      "         7582,  2087,  2083,  7536,  1312,  1875, 11109,  1365,  2117,  1312,\n",
      "         5267,  1305,  4274, 13461, 15846,  2402,  4829,  1305,  2734, 10861,\n",
      "         1312,  1317,  9041, 12046,  1363,  1285, 12745,  1312,  7309,  1319,\n",
      "        13738,  1305,  1287,  2535,  7915,  1363,  2018,  8292,  1300,  9914]) tensor([   2, 1800, 1503, 7635, 2294, 4596, 2454,   36,    3,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x[0],y[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 82280/82280 [00:02<00:00, 28053.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# пробежимся по итератору, чтобы убедиться что ничего не падает и он работает достаточно быстро\n",
    "\n",
    "progress_bar = tqdm(total=len(train_loader.dataset), desc='Testing')\n",
    "\n",
    "for x, y in train_loader:\n",
    "    progress_bar.update(x.size(0))\n",
    "    \n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 4330/4330 [00:00<00:00, 26364.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# пробежимся по итератору, чтобы убедиться что ничего не падает и он работает достаточно быстро\n",
    "\n",
    "progress_bar = tqdm(total=len(validation_loader.dataset), desc='Testing')\n",
    "\n",
    "for x, y in validation_loader:\n",
    "    progress_bar.update(x.size(0))\n",
    "    \n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@adam.wearne/seq2seq-with-pytorch-46dc00ff5164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Seq2Seq(nn.Module):\n",
    "#     def __init__(self, vocab_len, hidden_size):\n",
    "#         super(Seq2Seq, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.vocab_len = vocab_len\n",
    "        \n",
    "#         #encoder\n",
    "#         self.encoder_embedding = nn.Embedding(self.vocab_len, hidden_size)\n",
    "#         self.encoder_gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "#         #decoder \n",
    "#         self.decoder_embedding = nn.Embedding(self.hidden_size, vocab_len)\n",
    "#         self.decoder_gru = nn.GRU(hidden_size, hidden_size)\n",
    "#         self.decoder_out = nn.Linear(hidden_size, output_size)\n",
    "#         self.decoder_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "#     def forward(input_sequence, output_sequence):\n",
    "#         output = embedded\n",
    "        \n",
    "#         decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        \n",
    "#         encoder_outputs, hidden = self.encoder(input_tokens, input_lengths)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TEXT.vocab.stoi['<pad>']\n",
    "eos_idx = TEXT.vocab.stoi['<eos>']\n",
    "sos_idx = TEXT.vocab.stoi['<sos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>', '<UNK>', '<BOS>', '<EOS>', '▁', 'e', 't', 'a', 'i', 'n']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab().index(\"<PAD>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pretrained_embeddings = TEXT.vocab.vectors\n",
    "# model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "# UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "# model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
    "# model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
    "# model.embedding.weight.requires_grad = False\n",
    "# optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True]\n",
    "#                        , lr=1.0e-3)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 64/82280 [00:00<00:11, 7274.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,  3297,  1394,  1304, 13539, 12243,  1838,  4502,  4113,  8059,\n",
      "        13895,   100,    90,    65,   197,     9,  1444,   112,    76,  1348,\n",
      "            5,    42,    73,    62,    51,  2250,  1398,    47,  4690,  2117,\n",
      "         2811,  6982,  2156,  8762,  1352,  1306,  1959,  2339,  2087,  3046,\n",
      "         7582,  2087,  2083,  7536,  1312,  1875, 11109,  1365,  2117,  1312,\n",
      "         5267,  1305,  4274, 13461, 15846,  2402,  4829,  1305,  2734, 10861,\n",
      "         1312,  1317,  9041, 12046,  1363,  1285, 12745,  1312,  7309,  1319,\n",
      "        13738,  1305,  1287,  2535,  7915,  1363,  2018,  8292,  1300,  9914]) tensor([   2, 1800, 1503, 7635, 2294, 4596, 2454,   36,    3,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# пробежимся по итератору, чтобы убедиться что ничего не падает и он работает достаточно быстро\n",
    "\n",
    "progress_bar = tqdm(total=len(train_loader.dataset), desc='Testing')\n",
    "\n",
    "for x, y in train_loader:\n",
    "    print(x[0],y[0])\n",
    "    progress_bar.update(x.size(0))\n",
    "    break\n",
    "    \n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (model, iterator):\n",
    "    epoch_loss = 0\n",
    "    for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        with torch.no_grad():\n",
    "            input_seq = batch[0]\n",
    "            target_tokens = batch[1]\n",
    "            output = model(input_seq, target_tokens)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            target_tokens = target_tokens.view(-1)\n",
    "            print(\"AFTER RESHAPE PREDICTED_OUTPUT {}, REAL_OUTPUT {}\".format(output.shape, target_tokens.shape))\n",
    "            loss = criterion(output, target_tokens)\n",
    "            epoch_loss += loss.item()\n",
    "        break\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer):\n",
    "   # Put the model in training mode!\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    losses_list = []\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        optimizer.zero_grad()\n",
    "        input_seq = batch[0]\n",
    "        target_tokens = batch[1]\n",
    "#         print(len(batch), batch[0][0],batch[1][0])\n",
    "        output = model(input_seq, target_tokens)\n",
    "        \n",
    "        print(\"PREDICTED_OUTPUT {}, REAL_OUTPUT {}\".format(output.shape, target_tokens.shape))\n",
    "#         output = output.view(-1)\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        target_tokens = target_tokens.view(-1)\n",
    "        print(\"AFTER RESHAPE PREDICTED_OUTPUT {}, REAL_OUTPUT {}\".format(output.shape, target_tokens.shape))\n",
    "        loss = criterion(output, target_tokens)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        losses_list.append(loss.item())\n",
    "        \n",
    "        progress_bar.set_postfix(train_loss = np.mean(losses_list[-100:]))\n",
    "        break\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/41140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1800, 1503, 7635, 2294, 4596, 2454,   36,    3,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2294]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4596]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2454]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 2461, 1503, 7635, 9910, 1305, 1795, 2402, 1361, 5194, 1761,\n",
      "          36,    3,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2461]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9910]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1305]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1795]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2402]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1361]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5194]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1761]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 14th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "PREDICTED_OUTPUT torch.Size([2, 20, 16000]), REAL_OUTPUT torch.Size([2, 20])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/lilyakhoang/anaconda3/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type My_Seq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/lilyakhoang/anaconda3/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type EncoderRNN_inside_class. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/lilyakhoang/anaconda3/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type DecoderRNN_inside_class. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "  0%|          | 0/2165 [00:00<?, ?it/s]\n",
      "  0%|          | 0/41140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([    2,  1800,  1503,  9346,  2453,  1287,  5133,  4208,  6387,  1319,\n",
      "        11858,    36,     3,     0,     0,     0,     0,     0,     0,     0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2453]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5133]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4208]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[6387]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1319]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[11858]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 1503, 9346, 2955, 1522, 1439, 1287, 8088, 1886,    3,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2955]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1522]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1439]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[8088]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1886]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n",
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1800, 1503, 7635, 2294, 4596, 2454,   36,    3,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2294]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4596]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2454]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 2461, 1503, 7635, 9910, 1305, 1795, 2402, 1361, 5194, 1761,\n",
      "          36,    3,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2461]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9910]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1305]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1795]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2402]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1361]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5194]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1761]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 14th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "PREDICTED_OUTPUT torch.Size([2, 20, 16000]), REAL_OUTPUT torch.Size([2, 20])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2165 [00:00<?, ?it/s]\n",
      "  0%|          | 0/41140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([    2,  1800,  1503,  9346,  2453,  1287,  5133,  4208,  6387,  1319,\n",
      "        11858,    36,     3,     0,     0,     0,     0,     0,     0,     0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2453]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5133]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4208]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[6387]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1319]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[11858]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 1503, 9346, 2955, 1522, 1439, 1287, 8088, 1886,    3,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2955]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1522]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1439]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[8088]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1886]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n",
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1800, 1503, 7635, 2294, 4596, 2454,   36,    3,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2294]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4596]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2454]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 2461, 1503, 7635, 9910, 1305, 1795, 2402, 1361, 5194, 1761,\n",
      "          36,    3,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2461]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9910]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1305]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1795]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2402]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1361]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5194]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1761]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 14th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "PREDICTED_OUTPUT torch.Size([2, 20, 16000]), REAL_OUTPUT torch.Size([2, 20])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2165 [00:00<?, ?it/s]\n",
      "  0%|          | 0/41140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([    2,  1800,  1503,  9346,  2453,  1287,  5133,  4208,  6387,  1319,\n",
      "        11858,    36,     3,     0,     0,     0,     0,     0,     0,     0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2453]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5133]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4208]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[6387]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1319]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[11858]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 1503, 9346, 2955, 1522, 1439, 1287, 8088, 1886,    3,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2955]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1522]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1439]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[8088]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1886]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n",
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1800, 1503, 7635, 2294, 4596, 2454,   36,    3,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2294]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4596]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2454]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 2461, 1503, 7635, 9910, 1305, 1795, 2402, 1361, 5194, 1761,\n",
      "          36,    3,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2461]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9910]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1305]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1795]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2402]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1361]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5194]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1761]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 14th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "PREDICTED_OUTPUT torch.Size([2, 20, 16000]), REAL_OUTPUT torch.Size([2, 20])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2165 [00:00<?, ?it/s]\n",
      "  0%|          | 0/41140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([    2,  1800,  1503,  9346,  2453,  1287,  5133,  4208,  6387,  1319,\n",
      "        11858,    36,     3,     0,     0,     0,     0,     0,     0,     0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2453]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5133]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4208]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[6387]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1319]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[11858]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 1503, 9346, 2955, 1522, 1439, 1287, 8088, 1886,    3,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2955]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1522]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1439]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[8088]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1886]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n",
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1800, 1503, 7635, 2294, 4596, 2454,   36,    3,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2294]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4596]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2454]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 2461, 1503, 7635, 9910, 1305, 1795, 2402, 1361, 5194, 1761,\n",
      "          36,    3,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2461]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9910]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1305]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1795]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2402]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1361]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5194]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1761]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 14th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "PREDICTED_OUTPUT torch.Size([2, 20, 16000]), REAL_OUTPUT torch.Size([2, 20])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2165 [00:00<?, ?it/s]\n",
      "  0%|          | 0/41140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([    2,  1800,  1503,  9346,  2453,  1287,  5133,  4208,  6387,  1319,\n",
      "        11858,    36,     3,     0,     0,     0,     0,     0,     0,     0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2453]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5133]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4208]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[6387]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1319]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[11858]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 1503, 9346, 2955, 1522, 1439, 1287, 8088, 1886,    3,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2955]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1522]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1439]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[8088]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1886]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n",
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1800, 1503, 7635, 2294, 4596, 2454,   36,    3,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2294]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4596]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2454]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 2461, 1503, 7635, 9910, 1305, 1795, 2402, 1361, 5194, 1761,\n",
      "          36,    3,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2461]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9910]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1305]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1795]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2402]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1361]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5194]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1761]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 14th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "PREDICTED_OUTPUT torch.Size([2, 20, 16000]), REAL_OUTPUT torch.Size([2, 20])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2165 [00:00<?, ?it/s]\n",
      "  0%|          | 0/41140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([    2,  1800,  1503,  9346,  2453,  1287,  5133,  4208,  6387,  1319,\n",
      "        11858,    36,     3,     0,     0,     0,     0,     0,     0,     0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2453]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5133]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4208]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[6387]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1319]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[11858]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 1503, 9346, 2955, 1522, 1439, 1287, 8088, 1886,    3,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9346]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2955]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1522]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1439]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1287]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[8088]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1886]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n",
      "torch.Size([2, 20, 16000])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1800, 1503, 7635, 2294, 4596, 2454,   36,    3,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1800]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2294]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[4596]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2454]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "\n",
      " Start handling new element\n",
      "target_tensor.shape torch.Size([20]), target_tensor tensor([   2, 1411, 2461, 1503, 7635, 9910, 1305, 1795, 2402, 1361, 5194, 1761,\n",
      "          36,    3,    0,    0,    0,    0,    0,    0])\n",
      "go_to_decoder_with decoder_input tensor([[2]]), decoder_hidden.shape torch.Size([1, 1, 512])\n",
      "iterate_over_target 0th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2]])\n",
      "iterate_over_target 1th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1411]])\n",
      "iterate_over_target 2th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2461]])\n",
      "iterate_over_target 3th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1503]])\n",
      "iterate_over_target 4th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[7635]])\n",
      "iterate_over_target 5th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[9910]])\n",
      "iterate_over_target 6th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1305]])\n",
      "iterate_over_target 7th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1795]])\n",
      "iterate_over_target 8th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[2402]])\n",
      "iterate_over_target 9th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1361]])\n",
      "iterate_over_target 10th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[5194]])\n",
      "iterate_over_target 11th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[1761]])\n",
      "iterate_over_target 12th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[36]])\n",
      "iterate_over_target 13th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[3]])\n",
      "iterate_over_target 14th time, decoder_input.shape torch.Size([1, 1])\n",
      "AFTER_DECODER_STEP decoder_output.shape torch.Size([1, 16000]), decoder_input.shape torch.Size([1, 1]), decoder_input tensor([[0]])\n",
      "PREDICTED_OUTPUT torch.Size([2, 20, 16000]), REAL_OUTPUT torch.Size([2, 20])\n",
      "AFTER RESHAPE PREDICTED_OUTPUT torch.Size([40, 16000]), REAL_OUTPUT torch.Size([40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4361772e4ad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last_seq2seq\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-f85bdfbeac15>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, criterion, optimizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MY VERSIOB\n",
    "pad_idx = tokenizer.vocab().index(\"<PAD>\")\n",
    "eos_idx = tokenizer.vocab().index(\"<EOS>\")\n",
    "sos_idx = tokenizer.vocab().index(\"<BOS>\")\n",
    "# Size of embedding_dim should match the dim of pre-trained word embeddings!\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "vocab_size = len(tokenizer.vocab())\n",
    "model = My_Seq2Seq(embedding_dim,\n",
    "                 hidden_dim, \n",
    "                 vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=1.0e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "validation_losses =[]\n",
    "train_losses = []\n",
    "\n",
    "N_EPOCHS = 20\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if min(train_losses) == train_loss and len(train_losses) > 1:\n",
    "        torch.save(model.state_dict, \"best_seq2seq\")\n",
    "        torch.save(optimizer.state_dict, \"best_Adam_state_dict\")\n",
    "#     print(\"train_loss\",train_loss)\n",
    "    \n",
    "    torch.save(model.state_dict, \"last_seq2seq\")\n",
    "    torch.save(optimizer.state_dict, \"Adam_state_dict\")\n",
    "    #early stopping\n",
    "    test_loss = evaluate(model, validation_loader)\n",
    "    validation_losses.append(test_loss)\n",
    "    if len(validation_losses) > 1 and validation_losses[epoch] > validation_losses[epoch-1]:\n",
    "        print(\"stop\")\n",
    "        break\n",
    "#     print(\"test_loss\",test_loss)\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DEFAULT_train(model, iterator, criterion, optimizer, clip=1.0):\n",
    "   # Put the model in training mode!\n",
    "   model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        \n",
    "        input_sequence = batch.input_sequence\n",
    "        output_sequence = batch.output_sequence\n",
    "        \n",
    "        target_tokens = output_sequence[0]\n",
    "        \n",
    "        # zero out the gradient for the current batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run the batch through our model\n",
    "        output = model(input_sequence, output_sequence)\n",
    "        \n",
    "        # Throw it through our loss function\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        target_tokens = target_tokens[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, target_tokens)\n",
    "        \n",
    "        # Perform back-prop and calculate the gradient of our loss function\n",
    "        loss.backward()\n",
    "          \n",
    "        # Clip the gradient if necessary.          \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Seq2Seq(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx):\n",
    "        super(My_Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Encoder network\n",
    "        self.encoder = EncoderRNN_inside_class(hidden_size, \n",
    "                               embedding_size, \n",
    "                               self.embedding)\n",
    "        \n",
    "        # Decoder network        \n",
    "        self.decoder = DecoderRNN_inside_class(self.embedding,\n",
    "                               embedding_size,\n",
    "                              hidden_size,\n",
    "                              vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.device = device\n",
    "    def forward(self, input_sequence, output_sequence, debug = True):\n",
    "        input_tokens = input_sequence[0]\n",
    "        input_lengths = input_sequence[1]\n",
    "        \n",
    "        encoder_hidden = self.encoder.initHidden(len(input_sequence))\n",
    "        encoder_output, encoder_hidden = self.encoder(input_sequence, encoder_hidden)\n",
    "        batch_size = len(input_sequence)\n",
    "        outputs = torch.zeros(batch_size, 20, self.vocab_size).to(self.device)\n",
    "        print(outputs.shape)\n",
    "        for batch_element_index in range(batch_size):\n",
    "            target_tensor = output_sequence[batch_element_index, :]\n",
    "            if debug == True: \n",
    "                print(\"\\n Start handling new element\")\n",
    "                print(\"target_tensor.shape {}, target_tensor {}\".format(target_tensor.shape, target_tensor))\n",
    "\n",
    "            decoder_input = torch.tensor([[self.sos_idx]], device=device)\n",
    "            decoder_hidden = encoder_hidden[:,batch_element_index,:].unsqueeze(1)\n",
    "\n",
    "            if debug == True: \n",
    "                print(\"go_to_decoder_with decoder_input {}, decoder_hidden.shape {}\".format(decoder_input, decoder_hidden.shape))\n",
    "            \n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            for di in range(20):\n",
    "                if debug == True: \n",
    "                    print(\"iterate_over_target {}th time, decoder_input.shape {}\".format(di, decoder_input.shape))\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                outputs[batch_element_index, di, :] = decoder_output\n",
    "                decoder_input = torch.tensor([target_tensor[di]], device=device).unsqueeze(1)\n",
    "                if debug == True: \n",
    "                    print(\"AFTER_DECODER_STEP decoder_output.shape {}, decoder_input.shape {}, decoder_input {}\".format(decoder_output.shape, decoder_input.shape, decoder_input))\n",
    "                if decoder_input[0][0] == 0:\n",
    "                    break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN_inside_class(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, embedding):#input_size\n",
    "        super(EncoderRNN_inside_class, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True: \n",
    "            print(\"====ENCODING_FORWARD====\")\n",
    "            print(\"input.shape\", input.shape)\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        if debug == True: \n",
    "            print(\"embedded/output.shape\",embedded.shape,\"hidden.shape\", hidden.shape  )\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN_inside_class(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, embedding_size,\n",
    "                 hidden_size, output_size):\n",
    "        super(DecoderRNN_inside_class, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vectors = embedding\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True:\n",
    "          print(\"===FORWARD_DECODER===\")\n",
    "          print(\"input.shape {}, hidden.shape {}\".format(input.shape,hidden.shape ))\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TEXT.vocab.stoi['<pad>']\n",
    "eos_idx = TEXT.vocab.stoi['<eos>']\n",
    "sos_idx = TEXT.vocab.stoi['<sos>']\n",
    "# Size of embedding_dim should match the dim of pre-trained word embeddings!\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "vocab_size = len(TEXT.vocab)\n",
    "model = seq2seq(embedding_dim,\n",
    "                 hidden_dim, \n",
    "                 vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.requires_grad = False\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True]\n",
    "                       , lr=1.0e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEFAULT_seq2seq(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx, teacher_forcing_ratio=0.5):\n",
    "        super(seq2seq, self).__init__()\n",
    "        \n",
    "        # Embedding layer shared by encoder and decoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Encoder network\n",
    "        self.encoder = Encoder(hidden_size, \n",
    "                               embedding_size, \n",
    "                               self.embedding,\n",
    "                              num_layers=2,\n",
    "                              dropout=0.5)\n",
    "        \n",
    "        # Decoder network        \n",
    "        self.decoder = Decoder(self.embedding,\n",
    "                               embedding_size,\n",
    "                              hidden_size,\n",
    "                              vocab_size,\n",
    "                              n_layers=2,\n",
    "                              dropout=0.5)\n",
    "        \n",
    "        \n",
    "        # Indices of special tokens and hardware device \n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, input_sequence):\n",
    "        return (input_sequence != self.pad_idx).permute(1, 0)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_sequence, output_sequence, teacher_forcing_ratio=0.5):\n",
    "      \n",
    "        # Unpack input_sequence tuple\n",
    "        input_tokens = input_sequence[0]\n",
    "        input_lengths = input_sequence[1]\n",
    "      \n",
    "        # Unpack output_tokens, or create an empty tensor for text generation\n",
    "        if output_sequence is None:\n",
    "            inference = True\n",
    "            output_tokens = torch.zeros((100, input_tokens.shape[1])).long().fill_(self.sos_idx).to(self.device)\n",
    "        else:\n",
    "            inference = False\n",
    "            output_tokens = output_sequence[0]\n",
    "vocab_size = self.decoder.output_size\n",
    "        \n",
    "        batch_size = len(input_lengths)\n",
    "        max_seq_len = len(output_tokens)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_seq_len, batch_size, vocab_size).to(self.device)\n",
    "        \n",
    "        \n",
    "        # Pass through the first half of the network\n",
    "        encoder_outputs, hidden = self.encoder(input_tokens, input_lengths)\n",
    "        \n",
    "        # Ensure dim of hidden_state can be fed into Decoder\n",
    "        hidden =  hidden[:self.decoder.n_layers]\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = output_tokens[0,:]\n",
    "        \n",
    "        # Create mask\n",
    "        mask = self.create_mask(input_tokens)\n",
    "        \n",
    "        \n",
    "        # Step through the length of the output sequence one token at a time\n",
    "        # Teacher forcing is used to assist training\n",
    "        for t in range(1, max_seq_len):\n",
    "            output = output.unsqueeze(0)\n",
    "            \n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs, mask)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (output_tokens[t] if teacher_force else top1)\n",
    "            \n",
    "            # If we're in inference mode, keep generating until we produce an\n",
    "            # <eos> token\n",
    "            if inference and output.item() == self.eos_idx:\n",
    "                return outputs[:t]\n",
    "return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vectors=vectors):#input_size\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vectors = vectors\n",
    "        self.vocab_size, self.embedding_dim = vectors.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(self.vectors))\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True: \n",
    "            print(\"====ENCODING_FORWARD====\")\n",
    "            print(\"input.shape\", input.shape)\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        if debug == True: \n",
    "            print(\"embedded/output.shape\",embedded.shape,\"hidden.shape\", hidden.shape  )\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 64, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, vectors):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vectors = vectors\n",
    "        self.vocab_size, self.embedding_dim = vectors.shape\n",
    "        # self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(self.vectors))\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True:\n",
    "          print(\"===FORWARD_DECODER===\")\n",
    "          print(\"input.shape {}, hidden.shape {}\".format(input.shape,hidden.shape ))\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, max_len, pad_index, eos_index):\n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.eos_index = eos_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sequence = self.data[index][:self.max_len]\n",
    "        \n",
    "        # исходная последовательность\n",
    "        x = sequence[:]\n",
    "        # нужно предсказать смещенную последовательность\n",
    "        y = sequence[1:] + [self.eos_index]\n",
    "        \n",
    "        assert len(x) == len(y)\n",
    "        \n",
    "        pads = [self.pad_index] * (self.max_len - len(x))\n",
    "        \n",
    "        x = torch.tensor(x + pads).long()\n",
    "        y = torch.tensor(y + pads).long()\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(Dataset):\n",
    "    \n",
    "    def __init__(self, ctx_quest_pairs_list , word2index, context_sequence_length=80, \n",
    "                 question_sequence_length = 20, pad_token='PAD', verbose=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.x_data = []\n",
    "        self.y_data = []\n",
    "        \n",
    "        self.word2index = word2index\n",
    "        self.context_sequence_length = context_sequence_length\n",
    "        self.question_sequence_length = question_sequence_length\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.pad_index = self.word2index[self.pad_token]\n",
    "        \n",
    "        self.load(ctx_quest_pairs_list, verbose=verbose)\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_text(text):\n",
    "        \n",
    "        # Место для вашей предобработки\n",
    "    \n",
    "        words = wordpunct_tokenize(text.lower())\n",
    "\n",
    "        return words\n",
    "        \n",
    "    def load(self, data, verbose=True):\n",
    "        \n",
    "        data_iterator = tqdm(data, desc='Loading data', disable=not verbose)\n",
    "        \n",
    "        for ctx_quest_pair in data_iterator:\n",
    "            ctx = ctx_quest_pair[0]\n",
    "            quest = ctx_quest_pair[1]\n",
    "\n",
    "            ctx = self.process_text(ctx)\n",
    "            indexed_ctx = self.indexing(ctx)\n",
    "            self.x_data.append(indexed_ctx)\n",
    "\n",
    "            quest = self.process_text(quest)\n",
    "            indexed_quest = self.indexing(quest)\n",
    "            self.y_data.append(indexed_quest)\n",
    "    \n",
    "    def indexing(self, tokenized_text):\n",
    "        indexes = []\n",
    "        for word in tokenized_text:\n",
    "          if word in self.word2index:\n",
    "            indexes.append(self.word2index[word])\n",
    "        return indexes\n",
    "    \n",
    "    def padding(self, sequence, sequence_type):\n",
    "        if sequence_type == 'context':\n",
    "          sequence_length = self.context_sequence_length\n",
    "        elif sequence_type == \"question\":\n",
    "          sequence_length = self.question_sequence_length\n",
    "        else:\n",
    "          raise \"unkown sequence type\"\n",
    "        count = 0 \n",
    "        paded_seq = []\n",
    "        for seq_el in sequence:\n",
    "          paded_seq.append(seq_el)\n",
    "          count += 1\n",
    "          if count >= sequence_length: break\n",
    "        if count < sequence_length:\n",
    "          for ind in range (count, sequence_length):\n",
    "            paded_seq.append(self.pad_index)\n",
    "\n",
    "        return paded_seq\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = self.x_data[idx]\n",
    "        x = self.padding(x, \"context\")\n",
    "        x = torch.Tensor(x).long()\n",
    "\n",
    "        y = self.y_data[idx]\n",
    "        y = self.padding(y, \"question\")\n",
    "        y = torch.Tensor(y).long()\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# потом можете добавить свою предобработку\n",
    "\n",
    "def process_text(text):\n",
    "    \n",
    "    words = wordpunct_tokenize(text.lower())\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who did Antipater declar as his successor?',\n",
       " 'Which Tennessee Senator was the only Republican first-time Senator elected in 2006?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_data = []\n",
    "all_text_data.extend(list(df['context']))\n",
    "all_text_data.extend(list(df['question']))\n",
    "all_text_data = list(set(all_text_data))\n",
    "cleaned_text_data = [x for x in all_text_data if 'float' not in str(type(x))]\n",
    "cleaned_text_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c455a44eea4415a31301068c1ea65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=144855), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2freq = {}\n",
    "lengths = []\n",
    "\n",
    "for text in tqdm(cleaned_text_data):\n",
    "  \n",
    "    words = process_text(text)\n",
    "    \n",
    "    lengths.append(len(words))\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        if word in word2freq:\n",
    "            word2freq[word] += 1\n",
    "        else:\n",
    "            word2freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6f71e9754b68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.title('Распределение длин слов в текстах')\n",
    "plt.xlabel('Длина предложения')\n",
    "plt.ylabel('Доля')\n",
    "sns.distplot(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'66.57 % наших текстов входят в промежуток от 10 до 80 слов'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_threshold = 80\n",
    "lower_threshold = 10\n",
    "\n",
    "correct_percent = len([sent_len for sent_len in lengths \n",
    "                       if sent_len <= upper_threshold and sent_len >= lower_threshold]) * 100 / len(lengths)\n",
    "\n",
    "'{:.2f} % наших текстов входят в промежуток от {} до {} слов'.format(correct_percent, lower_threshold, upper_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LENGTH = 80\n",
    "QUESTION_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ЗАПУСТИТЬ ЕСЛИ ИЗ КОЛАБА\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the True 50\n",
      "['0.418', '0.24968', '-0.41242', '0.1217', '0.34527', '-0.044457', '-0.49688', '-0.17862', '-0.00066023', '-0.6566', '0.27843', '-0.14767', '-0.55677', '0.14658', '-0.0095095', '0.011658', '0.10204', '-0.12792', '-0.8443', '-0.12181', '-0.016801', '-0.33279', '-0.1552', '-0.23131', '-0.19181', '-1.8823', '-0.76746', '0.099051', '-0.42125', '-0.19526', '4.0071', '-0.18594', '-0.52287', '-0.31681', '0.00059213', '0.0074449', '0.17778', '-0.15897', '0.012041', '-0.054223', '-0.29871', '-0.15749', '-0.34758', '-0.045637', '-0.44251', '0.18785', '0.0027849', '-0.18411', '-0.11514', '-0.78581']\n"
     ]
    }
   ],
   "source": [
    "local_path = \"/Users/lilyakhoang/input/glove.6B/glove.6B.50d.txt\"\n",
    "this_folder_path = \"glove.6B.50d.txt\"\n",
    "with open(local_path, \"r\") as lines:\n",
    "    for line in lines:\n",
    "      print(line.split()[0], line.split()[0] in word2freq, len(line.split()[1:]))\n",
    "      print(line.split()[1:])\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec9e44586dc4ffe94ddeb6d4af5208c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Read word2vec', max=400000, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ИЗМЕНИТЬ ПУТЬ В ЗАВИСИМОСТИ ОТ ТОГО ГДЕ ЗАПУСКАЕМ КОД\n",
    "local_path = \"/Users/lilyakhoang/input/glove.6B/glove.6B.50d.txt\"\n",
    "this_folder_path = \"glove.6B.50d.txt\"\n",
    "\n",
    "word2index = {'PAD': 0}\n",
    "vectors = []\n",
    "   \n",
    "try:\n",
    "    lines = open(local_path)\n",
    "except:\n",
    "    lines = open(this_folder_path)\n",
    "    \n",
    "embedding_dim = 50\n",
    "# Zero vector for PAD\n",
    "vectors.append(np.zeros((1, embedding_dim)))\n",
    "progress_bar = tqdm(desc='Read word2vec', total=400000)\n",
    "\n",
    "for line in lines:\n",
    "    current_word = line.split()[0]\n",
    "    if current_word in word2freq:\n",
    "\n",
    "        word2index[current_word] = len(word2index)\n",
    "\n",
    "        # current_vectors = current_parts[-embedding_dim:]\n",
    "        current_vectors = line.split()[1:]\n",
    "        current_vectors = np.array(list(map(float, current_vectors)))\n",
    "        current_vectors = np.expand_dims(current_vectors, 0)\n",
    "\n",
    "        vectors.append(current_vectors)\n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "vectors = np.concatenate(vectors)\n",
    "\n",
    "lines.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мы не знаем 1.31 % слов в датасете\n",
      "Количество неизвестных слов 15247 из 80148, то есть 19.02 % уникальных слов в словаре\n",
      "В среднем каждое встречается 4.49 раз\n",
      "\n",
      "Топ 5 невошедших слов:\n",
      "), с количеством вхождениий - 8360\n",
      "). с количеством вхождениий - 6262\n",
      "\". с количеством вхождениий - 2903\n",
      "\", с количеством вхождениий - 2574\n",
      ".\" с количеством вхождениий - 2351\n"
     ]
    }
   ],
   "source": [
    "unk_words = [word for word in word2freq if word not in word2index]\n",
    "unk_counts = [word2freq[word] for word in unk_words]\n",
    "n_unk = sum(unk_counts) * 100 / sum(list(word2freq.values()))\n",
    "\n",
    "sub_sample_unk_words = {word: word2freq[word] for word in unk_words}\n",
    "sorted_unk_words = list(sorted(sub_sample_unk_words, key=lambda x: sub_sample_unk_words[x], reverse=True))\n",
    "\n",
    "print('Мы не знаем {:.2f} % слов в датасете'.format(n_unk))\n",
    "print('Количество неизвестных слов {} из {}, то есть {:.2f} % уникальных слов в словаре'.format(\n",
    "    len(unk_words), len(word2freq), len(unk_words) * 100 / len(word2freq)))\n",
    "print('В среднем каждое встречается {:.2f} раз'.format(np.mean(unk_counts)))\n",
    "print()\n",
    "print('Топ 5 невошедших слов:')\n",
    "\n",
    "for i in range(5):\n",
    "    print(sorted_unk_words[i], 'с количеством вхождениий -', word2freq[sorted_unk_words[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(Dataset):\n",
    "    \n",
    "    def __init__(self, ctx_quest_pairs_list , word2index, context_sequence_length=80, \n",
    "                 question_sequence_length = 20, pad_token='PAD', verbose=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.x_data = []\n",
    "        self.y_data = []\n",
    "        \n",
    "        self.word2index = word2index\n",
    "        self.context_sequence_length = context_sequence_length\n",
    "        self.question_sequence_length = question_sequence_length\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.pad_index = self.word2index[self.pad_token]\n",
    "        \n",
    "        self.load(ctx_quest_pairs_list, verbose=verbose)\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_text(text):\n",
    "        \n",
    "        # Место для вашей предобработки\n",
    "    \n",
    "        words = wordpunct_tokenize(text.lower())\n",
    "\n",
    "        return words\n",
    "        \n",
    "    def load(self, data, verbose=True):\n",
    "        \n",
    "        data_iterator = tqdm(data, desc='Loading data', disable=not verbose)\n",
    "        \n",
    "        for ctx_quest_pair in data_iterator:\n",
    "            ctx = ctx_quest_pair[0]\n",
    "            quest = ctx_quest_pair[1]\n",
    "\n",
    "            ctx = self.process_text(ctx)\n",
    "            indexed_ctx = self.indexing(ctx)\n",
    "            self.x_data.append(indexed_ctx)\n",
    "\n",
    "            quest = self.process_text(quest)\n",
    "            indexed_quest = self.indexing(quest)\n",
    "            self.y_data.append(indexed_quest)\n",
    "    \n",
    "    def indexing(self, tokenized_text):\n",
    "        indexes = []\n",
    "        for word in tokenized_text:\n",
    "          if word in self.word2index:\n",
    "            indexes.append(self.word2index[word])\n",
    "        return indexes\n",
    "    \n",
    "    def padding(self, sequence, sequence_type):\n",
    "        if sequence_type == 'context':\n",
    "          sequence_length = self.context_sequence_length\n",
    "        elif sequence_type == \"question\":\n",
    "          sequence_length = self.question_sequence_length\n",
    "        else:\n",
    "          raise \"unkown sequence type\"\n",
    "        count = 0 \n",
    "        paded_seq = []\n",
    "        for seq_el in sequence:\n",
    "          paded_seq.append(seq_el)\n",
    "          count += 1\n",
    "          if count >= sequence_length: break\n",
    "        if count < sequence_length:\n",
    "          for ind in range (count, sequence_length):\n",
    "            paded_seq.append(self.pad_index)\n",
    "\n",
    "        return paded_seq\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = self.x_data[idx]\n",
    "        x = self.padding(x, \"context\")\n",
    "        x = torch.Tensor(x).long()\n",
    "\n",
    "        y = self.y_data[idx]\n",
    "        y = self.padding(y, \"question\")\n",
    "        y = torch.Tensor(y).long()\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86610,\n",
       " (\"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time.\",\n",
       "  'When did Beyonce start becoming popular?'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_answer_pairs = []\n",
    "for ctx, quest in zip(list(df['context']), list(df['question'])):\n",
    "  if 'float' in str(type(ctx)) or 'float' in str(type(quest)):\n",
    "    continue\n",
    "  else:\n",
    "    cont_answer_pairs.append((ctx, quest))\n",
    "len(cont_answer_pairs), cont_answer_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155370d3cad14fe2bf107c1c7a74f344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading data', max=86610, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = WordData(cont_answer_pairs, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([33972, 33404, 16932,    11,  3127,   270,  8242,    11, 36992,    11,\n",
       "           200,    24,    23,   375,   437,   405,     2,  2852,    24,    14,\n",
       "            28,   137,  2195,     2,  9315,     2,   380,  1913,     6,  2844,\n",
       "             3,   375,     6,  1059,     7,  1971,     2,   736,     2,    66,\n",
       "          1767,     7,   904,  4027,     6,  5264,  5557,    19,     8,   951,\n",
       "             2,     6,   480,     5,  3101,     7,     1,   284,  2222,    19,\n",
       "           406,  2195,     4,  1886,  1146,  1535,  1728,    11,   126,  9697,\n",
       "            56,  1513,   951,     3,  1743,    21,    69,   623,     2, 23818]),\n",
       " tensor([   60,   116, 25922,   460,  1642,   803,   185,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 80]) torch.Size([64, 20])\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=64, drop_last = True)\n",
    "for x, y in data_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_p=0.1, max_length=QUESTION_LENGTH, vectors=vectors):#output_size\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.vectors = vectors\n",
    "        self.vocab_size, self.embedding_dim = vectors.shape\n",
    "        # self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(vectors, device = device))\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(self.vectors))\n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, 80)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "#         self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs, debug = False):\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "        if debug == True: \n",
    "            print(\"==forward_decoding_cycle===\")\n",
    "            print(\"input.shape\", input.shape)\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        if debug == True: print(\"embedded[0].shape {}, hidden.shape {}\".format(embedded[0].shape, hidden.shape))\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden), 1)), dim=1)\n",
    "        if debug == True: \n",
    "            print(\"attn_weights.shape\",attn_weights.shape,\"encoder_outputs.shape\", encoder_outputs.shape)\n",
    "            print(\"attn_weights.unsqueeze(0).shape\",attn_weights.unsqueeze(0).shape,\"encoder_outputs.unsqueeze(0).shape\", encoder_outputs.unsqueeze(0).shape)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        if debug == True: \n",
    "            print(\"output.shape {}, hidden.shape {}\".format(output.shape,hidden.shape ))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        hidden = hidden.squeeze(1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vectors=vectors):#input_size\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vectors = vectors\n",
    "        self.vocab_size, self.embedding_dim = vectors.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(self.vectors))\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True: \n",
    "            print(\"====ENCODING_FORWARD====\")\n",
    "            print(\"input.shape\", input.shape)\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        if debug == True: \n",
    "            print(\"embedded/output.shape\",embedded.shape,\"hidden.shape\", hidden.shape  )\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 64, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, vectors):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vectors = vectors\n",
    "        self.vocab_size, self.embedding_dim = vectors.shape\n",
    "        # self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(self.vectors))\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True:\n",
    "          print(\"===FORWARD_DECODER===\")\n",
    "          print(\"input.shape {}, hidden.shape {}\".format(input.shape,hidden.shape ))\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_easy_attention(input_tensor, target_batch_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, ctx_len =SENTENCE_LENGTH, \n",
    "          debug = False):\n",
    "    if debug == True: print(\"target_batch_tensor.shape {}\".format(target_batch_tensor.shape))\n",
    "    \n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_batch_tensor.size(1)\n",
    "    \n",
    "\n",
    "    encoder_outputs_storage = torch.zeros(ctx_len, encoder.hidden_size, device=device)\n",
    "    if debug == True: \n",
    "        print(\"target_length\", target_length)\n",
    "        print(\"encoder_outputs.shape\", encoder_outputs.shape)\n",
    "        print(\"input_length is\", input_length)\n",
    "        \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "    \n",
    "    if debug == True: print(\"AFTER_ENCODING , encoder_output.shape{}, encoder_hidden.shape{}\".format(encoder_output.shape, encoder_hidden.shape))\n",
    "    \n",
    "    for batch_element_index in range(64):\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        target_tensor = target_batch_tensor[batch_element_index, :]\n",
    "        target_tensor = target_tensor.unsqueeze(-1)\n",
    "        if debug == True: \n",
    "            print(\"target_tensor.shape {}, target_tensor {}\".format(target_tensor.shape, target_tensor))\n",
    "            \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "#         decoder_hidden = encoder_hidden[:,batch_element_index,:].unsqueeze(1)\n",
    "        decoder_hidden = encoder_hidden[:,batch_element_index,:]\n",
    "        \n",
    "        encoder_output_in_current_index = encoder_output[batch_element_index,:,:]\n",
    "        if debug == True: \n",
    "            print(\"go_to_decoder_with decoder_input{}, decoder_hidden.shape {}\".format(decoder_input, decoder_hidden.shape))\n",
    "\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            if debug == True: \n",
    "                print(\"iterate_over_target {}th time, decoder_input.shape {}\".format(di, decoder_input.shape))\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output_in_current_index)\n",
    "            if debug == True: \n",
    "                print(\"AFTER_DECODER_STEP decoder_output.shape {}, decoder_input.shape {}, decoder_input {}\".format(decoder_output.shape, decoder_input.shape, decoder_input))\n",
    "#                 decoder_input = torch.tensor([decoder_input], device=device)\n",
    "            target_word = torch.tensor([target_tensor[di]], device=device)\n",
    "            loss += criterion(decoder_output, target_word)\n",
    "            decoder_input = torch.tensor([target_tensor[di]], device=device).unsqueeze(1)  # Teacher forcing\n",
    "#                 decoder_input = target_tensor[di]\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_easy(input_tensor, target_batch_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, ctx_len =SENTENCE_LENGTH, \n",
    "          debug = False):\n",
    "    if debug == True: print(\"target_batch_tensor.shape {}\".format(target_batch_tensor.shape))\n",
    "    \n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_batch_tensor.size(1)\n",
    "    if debug == True: print(\"target_length\", target_length)\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "    \n",
    "\n",
    "    if debug == True: print(\"AFTER_ENCODING , encoder_output.shape{}, encoder_hidden.shape{}\".format(encoder_output.shape, encoder_hidden.shape))\n",
    "    \n",
    "    for batch_element_index in range(64):\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        target_tensor = target_batch_tensor[batch_element_index, :]\n",
    "        if debug == True: \n",
    "            print(\"target_tensor.shape {}, target_tensor {}\".format(target_tensor.shape, target_tensor))\n",
    "            \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden[:,batch_element_index,:].unsqueeze(1)\n",
    "        \n",
    "        if debug == True: \n",
    "            print(\"go_to_decoder_with decoder_input{}, decoder_hidden.shape {}\".format(decoder_input, decoder_hidden.shape))\n",
    "\n",
    "\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            if debug == True: \n",
    "                print(\"iterate_over_target {}th time, decoder_input.shape {}\".format(di, decoder_input.shape))\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            if debug == True: \n",
    "                print(\"AFTER_DECODER_STEP decoder_output.shape {}, decoder_input.shape {}, decoder_input {}\".format(decoder_output.shape, decoder_input.shape, decoder_input))\n",
    "            target_word = torch.tensor([target_tensor[di]], device=device)\n",
    "            loss += criterion(decoder_output, target_word)\n",
    "            decoder_input = torch.tensor([target_tensor[di]], device=device).unsqueeze(1)  # Teacher forcing\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters_loader(encoder, decoder, epochs = 5, learning_rate=0.01, total_items = len(cont_answer_pairs), debug = False, attention = False):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = []\n",
    "    criterion = nn.NLLLoss()\n",
    "    losses = []\n",
    "    \n",
    "    for n_epoch in range(epochs):\n",
    "      progress_bar = tqdm(total=total_items, desc='Epoch {}'.format(n_epoch + 1))\n",
    "      for input_tensor, target_tensor in data_loader:\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            target_tensor = target_tensor.to(device)\n",
    "            if debug == True:print(\"input_tensor.shape\", input_tensor.shape, \"target_tensor.shape\", target_tensor.shape)\n",
    "            if attention == False:\n",
    "                loss = train_easy(input_tensor, target_tensor, encoder,\n",
    "                          decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            else:\n",
    "                loss = train_easy_attention(input_tensor, target_tensor, encoder,\n",
    "                          decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            losses.append(loss)\n",
    "            progress_bar.set_postfix(train_loss = np.mean(losses[-100:]))\n",
    "            progress_bar.update(input_tensor.shape[0])\n",
    "            print_loss_total += loss\n",
    "            \n",
    "            \n",
    "            input_tensor = input_tensor.to(device)\n",
    "            target_tensor = target_tensor.to(device)\n",
    "            if debug == True:print(\"input_tensor.shape\", input_tensor.shape, \"target_tensor.shape\", target_tensor.shape)\n",
    "            if attention == False:\n",
    "                print(\"no_attent\")\n",
    "                loss = train_easy(input_tensor, target_tensor, encoder,\n",
    "                          decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            else:\n",
    "                print(\"attent\")\n",
    "                loss = train_easy_attention(input_tensor, target_tensor, encoder,\n",
    "                          decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            losses.append(loss)\n",
    "            progress_bar.set_postfix(train_loss = np.mean(losses[-100:]))\n",
    "            progress_bar.update(input_tensor.shape[0])\n",
    "            print_loss_total += loss\n",
    "\n",
    "    print_loss_avg = print_loss_total / total_items\n",
    "    # tqdm.write('Losses: train - {:.3f}, test = {:.3f}'.format(np.mean(train_losses), mean_test_loss))\n",
    "    tqdm.write('Losses: train - {:.3f}'.format(print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN WITHOUT ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embedding): Embedding(64902, 50)\n",
       "  (gru): GRU(50, 50)\n",
       "  (out): Linear(in_features=50, out_features=64902, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 50\n",
    "# 37487\n",
    "encoder1 = EncoderRNN(hidden_size)\n",
    "encoder1.to(device)\n",
    "# attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, max_length = MAX_LENGTH, dropout_p=0.1).to(device)\n",
    "# attn_decoder1 = AttnDecoderRNN(hidden_size, dropout_p=0.1)\n",
    "attn_decoder1 = DecoderRNN(hidden_size, vectors)\n",
    "attn_decoder1.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d439dfaf2b42658d4edbe5f4a8bd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=86610, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-57b68b249ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainIters_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-9b8d3f8ed14d>\u001b[0m in \u001b[0;36mtrainIters_loader\u001b[0;34m(encoder, decoder, epochs, learning_rate, total_items, debug, attention)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 loss = train_easy(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 19\u001b[0;31m                           decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 loss = train_easy_attention(input_tensor, target_tensor, encoder,\n",
      "\u001b[0;32m<ipython-input-22-591f657afb08>\u001b[0m in \u001b[0;36mtrain_easy\u001b[0;34m(input_tensor, target_batch_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, ctx_len, debug)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainIters_loader(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(encoder1, 'Encoder_no_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DecoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(attn_decoder1, 'Decoder_no_attention_5_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN WITH ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefef10d909b4a39814932e0f46da538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=86610, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-545fc65658fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainIters_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattanetion_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_attention_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-6f41907d549e>\u001b[0m in \u001b[0;36mtrainIters_loader\u001b[0;34m(encoder, decoder, epochs, learning_rate, total_items, debug, attention)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 loss = train_easy_attention(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 22\u001b[0;31m                           decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-37b98b09d3fe>\u001b[0m in \u001b[0;36mtrain_easy_attention\u001b[0;34m(input_tensor, target_batch_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, ctx_len, debug)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#                 decoder_input = target_tensor[di]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 50\n",
    "# encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attanetion_encoder = EncoderRNN(hidden_size)\n",
    "attanetion_encoder.to(device)\n",
    "real_attention_decoder = AttnDecoderRNN(hidden_size, dropout_p=0.1)\n",
    "real_attention_decoder.to(device)\n",
    "\n",
    "trainIters_loader(attanetion_encoder, real_attention_decoder, attention = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
