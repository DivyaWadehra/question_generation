{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import youtokentome as yttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            context  \\\n",
       "0           0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1           1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2           2  Managed by her father, Mathew Knowles, the gro...   \n",
       "3           3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4           4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question  \n",
       "0           When did Beyonce start becoming popular?  \n",
       "1  What areas did Beyonce compete in when she was...  \n",
       "2  When did Beyonce leave Destiny's Child and bec...  \n",
       "3      In what city and state did Beyonce  grow up?   \n",
       "4         In which decade did Beyonce become famous?  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ИЗМЕНИТЬ ПУТЬ В ЗАВИСИМОСТИ ОТ ТОГО ГДЕ ЗАПУСКАЕМ КОД\n",
    "try:\n",
    "    df = pd.read_csv(\"ctx_quest.csv\")\n",
    "except:\n",
    "    df = pd.read_csv(\"/Users/lilyakhoang/input/question_generation/ctx_quest.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 86610 entries, 0 to 86820\n",
      "Data columns (total 3 columns):\n",
      "Unnamed: 0    86610 non-null int64\n",
      "context       86610 non-null object\n",
      "question      86610 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_nonan = df.dropna()\n",
    "df_nonan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# для \"обучения\" bpe модели нам нужно сохранить данные для обучения в отдельный файл\n",
    "# где будут построчно храниться тексты\n",
    "err = 0\n",
    "with open('/Users/lilyakhoang/input/for_bpe_ctx_quest.txt', 'w') as f:\n",
    "    for que in df_nonan.context:\n",
    "        try:\n",
    "            f.write(que + '\\n')\n",
    "        except:\n",
    "            err += 1\n",
    "    for que in df_nonan.question:\n",
    "        try:\n",
    "            f.write(que + '\\n')\n",
    "        except:\n",
    "            err += 1\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x1a15c260b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучаем\n",
    "vocab_size = 16000\n",
    "model_path = 'bpe.model'\n",
    "\n",
    "yttm.BPE.train(data='/Users/lilyakhoang/input/for_bpe_ctx_quest.txt', vocab_size=vocab_size, model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6507e7ccf5b94e99bded6487e2e869fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=339), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b8db8b899e475d8bd4d6f8b215b100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=339), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# давайте токенизируем наш датасет\n",
    "# токенизирую батчами, потому что так быстрее\n",
    "# также в начало добавляем токен bos (begin of sentence)\n",
    "\n",
    "tokenized_ctx = []\n",
    "tokenized_quest = []\n",
    "batch_size = 256\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(df_nonan.context) / batch_size))):\n",
    "    \n",
    "    tokenized_ctx.extend(tokenizer.encode(list(df_nonan.context[i_batch*batch_size:(i_batch+1)*batch_size]), bos=True))\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(df_nonan.question) / batch_size))):\n",
    "\n",
    "    tokenized_quest.extend(tokenizer.encode(list(df_nonan.question[i_batch*batch_size:(i_batch+1)*batch_size]), bos = True, eos=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " '<UNK>',\n",
       " '<BOS>',\n",
       " '<EOS>',\n",
       " '▁',\n",
       " 'e',\n",
       " 't',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 'h',\n",
       " 'l']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(torch.utils.data.Dataset):\n",
    "    def __init__(self, context_list, questions_list, context_len, questions_len, pad_index, eos_index):\n",
    "        self.context_list = context_list\n",
    "        self.questions_list = questions_list\n",
    "        \n",
    "        self.context_len = context_len\n",
    "        self.questions_len = questions_len\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.eos_index = eos_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.context_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        context = self.context_list[index][:self.context_len]\n",
    "        pads_ctx = [self.pad_index] * (self.context_len - len(context))\n",
    "#         print(len(pads_ctx))\n",
    "        context = torch.tensor(context + pads_ctx).long()\n",
    "        \n",
    "        question = self.questions_list[index][:self.questions_len]\n",
    "        pads_quest = [self.pad_index] * (self.questions_len - len(question))\n",
    "        question = torch.tensor(question + pads_quest).long()\n",
    "        \n",
    "        return context, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "context_len = 80\n",
    "quest_len = 20\n",
    "\n",
    "pad_index = 0\n",
    "eos_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start_index = int(len(tokenized_ctx) * 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285, 15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_size = 300\n",
    "small_ex_ctx = tokenized_ctx[:subset_size]\n",
    "small_ex_quest = tokenized_quest[:subset_size]\n",
    "small_validation_start_index = int(len(small_ex_ctx) *0.05)\n",
    "\n",
    "small_train_dataset = WordData(context_list=small_ex_ctx[:-small_validation_start_index], questions_list = small_ex_quest[:-small_validation_start_index],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "small_validation_dataset = WordData(context_list=small_ex_ctx[-small_validation_start_index:],questions_list = small_ex_quest[-small_validation_start_index:],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "len(small_train_dataset), len(small_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(small_train_dataset, batch_size=64)\n",
    "validation_loader = torch.utils.data.DataLoader(small_validation_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WordData(context_list=tokenized_ctx[:-validation_start_index], questions_list = tokenized_quest[:-validation_start_index],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "validation_dataset = WordData(context_list=tokenized_ctx[-validation_start_index:],questions_list = tokenized_quest[-validation_start_index:],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN_inside_class(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, embedding):#input_size\n",
    "        super(EncoderRNN_inside_class, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True: \n",
    "            print(\"====ENCODING_FORWARD====\")\n",
    "            print(\"input.shape\", input.shape)\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        if debug == True: \n",
    "            print(\"embedded/output.shape\",embedded.shape,\"hidden.shape\", hidden.shape  )\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN_inside_class(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, embedding_size,\n",
    "                 hidden_size, output_size):\n",
    "        super(DecoderRNN_inside_class, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vectors = embedding\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True:\n",
    "          print(\"===FORWARD_DECODER===\")\n",
    "          print(\"input.shape {}, hidden.shape {}\".format(input.shape,hidden.shape ))\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Seq2Seq(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx):\n",
    "        super(My_Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Encoder network\n",
    "        self.encoder = EncoderRNN_inside_class(hidden_size, \n",
    "                               embedding_size, \n",
    "                               self.embedding)\n",
    "        \n",
    "        # Decoder network        \n",
    "        self.decoder = DecoderRNN_inside_class(self.embedding,\n",
    "                               embedding_size,\n",
    "                              hidden_size,\n",
    "                              vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.device = device\n",
    "    def forward(self, input_sequence, output_sequence, debug = False):\n",
    "        input_tokens = input_sequence[0]\n",
    "        input_lengths = input_sequence[1]\n",
    "        \n",
    "        encoder_hidden = self.encoder.initHidden(len(input_sequence))\n",
    "        encoder_output, encoder_hidden = self.encoder(input_sequence, encoder_hidden)\n",
    "        batch_size = len(input_sequence)\n",
    "        outputs = torch.zeros(batch_size, 20, self.vocab_size).to(self.device)\n",
    "#         print(outputs.shape)\n",
    "        for batch_element_index in range(batch_size):\n",
    "            target_tensor = output_sequence[batch_element_index, :]\n",
    "            if debug == True: \n",
    "                print(\"\\n Start handling new element\")\n",
    "                print(\"target_tensor.shape {}, target_tensor {}\".format(target_tensor.shape, target_tensor))\n",
    "\n",
    "            decoder_input = torch.tensor([[self.sos_idx]], device=device)\n",
    "            decoder_hidden = encoder_hidden[:,batch_element_index,:].unsqueeze(1)\n",
    "\n",
    "            if debug == True: \n",
    "                print(\"go_to_decoder_with decoder_input {}, decoder_hidden.shape {}\".format(decoder_input, decoder_hidden.shape))\n",
    "            \n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            for di in range(20):\n",
    "                if debug == True: \n",
    "                    print(\"iterate_over_target {}th time, decoder_input.shape {}\".format(di, decoder_input.shape))\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                outputs[batch_element_index, di, :] = decoder_output\n",
    "                decoder_input = torch.tensor([target_tensor[di]], device=device).unsqueeze(1)\n",
    "                if debug == True: \n",
    "                    print(\"AFTER_DECODER_STEP decoder_output.shape {}, decoder_input.shape {}, decoder_input {}\".format(decoder_output.shape, decoder_input.shape, decoder_input))\n",
    "                if decoder_input[0][0] == 0:\n",
    "                    break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (model, iterator):\n",
    "    epoch_loss = 0\n",
    "    for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        with torch.no_grad():\n",
    "            input_seq = batch[0]\n",
    "            target_tokens = batch[1]\n",
    "            output = model(input_seq, target_tokens)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            target_tokens = target_tokens.view(-1)\n",
    "#             print(\"AFTER RESHAPE PREDICTED_OUTPUT {}, REAL_OUTPUT {}\".format(output.shape, target_tokens.shape))\n",
    "            loss = criterion(output, target_tokens)\n",
    "            epoch_loss += loss.item()\n",
    "        break\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer,epoch_number, debug = False):\n",
    "   # Put the model in training mode!\n",
    "    model.train()\n",
    "    losses_list = []\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(iterator), total=len(iterator), desc='Epoch {}'.format(epoch_number + 1))\n",
    "    for idx, batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_seq = batch[0]\n",
    "        target_tokens = batch[1]\n",
    "#         print(len(batch), batch[0][0],batch[1][0])\n",
    "        output = model(input_seq, target_tokens)\n",
    "        if debug == True:\n",
    "            print(\"PREDICTED_OUTPUT {}, REAL_OUTPUT {}\".format(output.shape, target_tokens.shape))\n",
    "#         output = output.view(-1)\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        target_tokens = target_tokens.view(-1)\n",
    "        if debug == True:\n",
    "            print(\"AFTER RESHAPE PREDICTED_OUTPUT {}, REAL_OUTPUT {}\".format(output.shape, target_tokens.shape))\n",
    "        loss = criterion(output, target_tokens)\n",
    "        losses_list.append(float(loss))\n",
    "        progress_bar.set_postfix(train_loss = np.mean(losses_list[-100:]))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "#         break\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1880638da55452096816262cedb30b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=5, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2686321a3709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-11a0371f9975>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, criterion, optimizer, epoch_number, debug)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlosses_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MY VERSIOB\n",
    "pad_idx = tokenizer.vocab().index(\"<PAD>\")\n",
    "eos_idx = tokenizer.vocab().index(\"<EOS>\")\n",
    "sos_idx = tokenizer.vocab().index(\"<BOS>\")\n",
    "# Size of embedding_dim should match the dim of pre-trained word embeddings!\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "vocab_size = len(tokenizer.vocab())\n",
    "model = My_Seq2Seq(embedding_dim,\n",
    "                 hidden_dim, \n",
    "                 vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=1.0e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "validation_losses =[]\n",
    "train_losses = []\n",
    "\n",
    "N_EPOCHS = 20\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if min(train_losses) == train_loss and len(train_losses) > 1:\n",
    "        torch.save(model.state_dict, \"best_seq2seq\")\n",
    "        torch.save(optimizer.state_dict, \"best_Adam_state_dict\")\n",
    "#     print(\"train_loss\",train_loss)\n",
    "    \n",
    "    torch.save(model.state_dict, \"last_seq2seq\")\n",
    "    torch.save(optimizer.state_dict, \"Adam_state_dict\")\n",
    "    #early stopping\n",
    "    test_loss = evaluate(model, validation_loader)\n",
    "    validation_losses.append(test_loss)\n",
    "    \n",
    "    if len(validation_losses) > 1 and validation_losses[epoch] > validation_losses[epoch-1]:\n",
    "        print(\"stop\")\n",
    "        break\n",
    "#     print(\"test_loss\",test_loss)\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
