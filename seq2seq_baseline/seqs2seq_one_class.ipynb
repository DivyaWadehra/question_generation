{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.40.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.4.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.13.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.13.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.3)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.1.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.4.0)\n",
      "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.17.3)\n",
      "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (0.25.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (2.4.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2019.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=1.4.3->seaborn) (1.13.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (41.6.0)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
      "Collecting youtokentome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/16/4cb7a9358430996bd6fa7daf32421105fe37a7bd0e4da1f79496e15aa509/youtokentome-1.0.5-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7MB 15.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Click>=7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 20.1MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: Click, youtokentome\n",
      "Successfully installed Click-7.0 youtokentome-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install tqdm\n",
    "!pip3 install nltk\n",
    "!pip3 install pandas\n",
    "!pip3 install numpy\n",
    "!pip3 install seaborn\n",
    "!apt-get install wget\n",
    "!pip3 install youtokentome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import youtokentome as yttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            context  \\\n",
       "0           0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1           1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2           2  Managed by her father, Mathew Knowles, the gro...   \n",
       "3           3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4           4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question  \n",
       "0           When did Beyonce start becoming popular?  \n",
       "1  What areas did Beyonce compete in when she was...  \n",
       "2  When did Beyonce leave Destiny's Child and bec...  \n",
       "3      In what city and state did Beyonce  grow up?   \n",
       "4         In which decade did Beyonce become famous?  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ИЗМЕНИТЬ ПУТЬ В ЗАВИСИМОСТИ ОТ ТОГО ГДЕ ЗАПУСКАЕМ КОД\n",
    "try:\n",
    "    df = pd.read_csv(\"ctx_quest.csv\")\n",
    "except:\n",
    "    df = pd.read_csv(\"/Users/lilyakhoang/input/question_generation/ctx_quest.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 86610 entries, 0 to 86820\n",
      "Data columns (total 3 columns):\n",
      "Unnamed: 0    86610 non-null int64\n",
      "context       86610 non-null object\n",
      "question      86610 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_nonan = df.dropna()\n",
    "df_nonan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# для \"обучения\" bpe модели нам нужно сохранить данные для обучения в отдельный файл\n",
    "# где будут построчно храниться тексты\n",
    "err = 0\n",
    "try:\n",
    "    f = open('/Users/lilyakhoang/input/for_bpe_ctx_quest.txt', 'w')\n",
    "    bpe_model_address = '/Users/lilyakhoang/input/for_bpe_ctx_quest.txt'\n",
    "except:\n",
    "    f = open('for_bpe_ctx_quest.txt', 'w')\n",
    "    bpe_model_address = 'for_bpe_ctx_quest.txt'\n",
    "for que in df_nonan.context:\n",
    "    try:\n",
    "        f.write(que + '\\n')\n",
    "    except:\n",
    "        err += 1\n",
    "for que in df_nonan.question:\n",
    "    try:\n",
    "        f.write(que + '\\n')\n",
    "    except:\n",
    "        err += 1\n",
    "f.close()\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x7f17d186bbe0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучаем\n",
    "vocab_size = 16000\n",
    "model_path = 'bpe.model'\n",
    "\n",
    "yttm.BPE.train(data=bpe_model_address,vocab_size=vocab_size, model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028a9d718c244da8930e3e0f54e7bae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=339.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c1ba8de75d4d36bc2f3e8289de6aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=339.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# давайте токенизируем наш датасет\n",
    "# токенизирую батчами, потому что так быстрее\n",
    "# также в начало добавляем токен bos (begin of sentence)\n",
    "\n",
    "tokenized_ctx = []\n",
    "tokenized_quest = []\n",
    "batch_size = 256\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(df_nonan.context) / batch_size))):\n",
    "    \n",
    "    tokenized_ctx.extend(tokenizer.encode(list(df_nonan.context[i_batch*batch_size:(i_batch+1)*batch_size]), bos=True))\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(df_nonan.question) / batch_size))):\n",
    "\n",
    "    tokenized_quest.extend(tokenizer.encode(list(df_nonan.question[i_batch*batch_size:(i_batch+1)*batch_size]), bos = True, eos=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " '<UNK>',\n",
       " '<BOS>',\n",
       " '<EOS>',\n",
       " '▁',\n",
       " 'e',\n",
       " 't',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 'h',\n",
       " 'l']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(torch.utils.data.Dataset):\n",
    "    def __init__(self, context_list, questions_list, context_len, questions_len, pad_index, eos_index):\n",
    "        self.context_list = context_list\n",
    "        self.questions_list = questions_list\n",
    "        \n",
    "        self.context_len = context_len\n",
    "        self.questions_len = questions_len\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.eos_index = eos_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.context_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        context = self.context_list[index][:self.context_len]\n",
    "        pads_ctx = [self.pad_index] * (self.context_len - len(context))\n",
    "#         print(len(pads_ctx))\n",
    "        context = torch.tensor(context + pads_ctx).long()\n",
    "        \n",
    "        question = self.questions_list[index][:self.questions_len]\n",
    "        pads_quest = [self.pad_index] * (self.questions_len - len(question))\n",
    "        question = torch.tensor(question + pads_quest).long()\n",
    "        \n",
    "        return context, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "context_len = 80\n",
    "quest_len = 20\n",
    "\n",
    "pad_index = 0\n",
    "eos_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start_index = int(len(tokenized_ctx) * 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285, 15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_size = 300\n",
    "small_ex_ctx = tokenized_ctx[:subset_size]\n",
    "small_ex_quest = tokenized_quest[:subset_size]\n",
    "small_validation_start_index = int(len(small_ex_ctx) *0.05)\n",
    "\n",
    "small_train_dataset = WordData(context_list=small_ex_ctx[:-small_validation_start_index], questions_list = small_ex_quest[:-small_validation_start_index],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "small_validation_dataset = WordData(context_list=small_ex_ctx[-small_validation_start_index:],questions_list = small_ex_quest[-small_validation_start_index:],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "len(small_train_dataset), len(small_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82280, 4330)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = WordData(context_list=tokenized_ctx[:-validation_start_index], questions_list = tokenized_quest[:-validation_start_index],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "validation_dataset = WordData(context_list=tokenized_ctx[-validation_start_index:],questions_list = tokenized_quest[-validation_start_index:],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(small_train_dataset, batch_size=64)\n",
    "validation_loader = torch.utils.data.DataLoader(small_validation_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82280, 4330)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = WordData(context_list=tokenized_ctx[:-validation_start_index], questions_list = tokenized_quest[:-validation_start_index],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "validation_dataset = WordData(context_list=tokenized_ctx[-validation_start_index:],questions_list = tokenized_quest[-validation_start_index:],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN_inside_class(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, embedding):#input_size\n",
    "        super(EncoderRNN_inside_class, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True: \n",
    "            print(\"====ENCODING_FORWARD====\")\n",
    "            print(\"input.shape\", input.shape)\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        if debug == True: \n",
    "            print(\"embedded/output.shape\",embedded.shape,\"hidden.shape\", hidden.shape  )\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN_inside_class(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, embedding_size,\n",
    "                 hidden_size, output_size):\n",
    "        super(DecoderRNN_inside_class, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vectors = embedding\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True:\n",
    "          print(\"===FORWARD_DECODER===\")\n",
    "          print(\"input.shape {}, hidden.shape {}\".format(input.shape,hidden.shape ))\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Seq2Seq(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx):\n",
    "        super(My_Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Encoder network\n",
    "        self.encoder = EncoderRNN_inside_class(hidden_size, \n",
    "                               embedding_size, \n",
    "                               self.embedding)\n",
    "        \n",
    "        # Decoder network        \n",
    "        self.decoder = DecoderRNN_inside_class(self.embedding,\n",
    "                               embedding_size,\n",
    "                              hidden_size,\n",
    "                              vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.device = device\n",
    "    def forward(self, input_sequence, output_sequence, debug = False):\n",
    "        input_tokens = input_sequence[0]\n",
    "        input_lengths = input_sequence[1]\n",
    "        \n",
    "        encoder_hidden = self.encoder.initHidden(len(input_sequence))\n",
    "        encoder_output, encoder_hidden = self.encoder(input_sequence, encoder_hidden)\n",
    "        batch_size = len(input_sequence)\n",
    "        outputs = torch.zeros(batch_size, 20, self.vocab_size).to(self.device)\n",
    "#         print(outputs.shape)\n",
    "        for batch_element_index in range(batch_size):\n",
    "            target_tensor = output_sequence[batch_element_index, :]\n",
    "            if debug == True: \n",
    "                print(\"\\n Start handling new element\")\n",
    "                print(\"target_tensor.shape {}, target_tensor {}\".format(target_tensor.shape, target_tensor))\n",
    "\n",
    "            decoder_input = torch.tensor([[self.sos_idx]], device=device)\n",
    "            decoder_hidden = encoder_hidden[:,batch_element_index,:].unsqueeze(1)\n",
    "\n",
    "            if debug == True: \n",
    "                print(\"go_to_decoder_with decoder_input {}, decoder_hidden.shape {}\".format(decoder_input, decoder_hidden.shape))\n",
    "            \n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            for di in range(20):\n",
    "                if debug == True: \n",
    "                    print(\"iterate_over_target {}th time, decoder_input.shape {}\".format(di, decoder_input.shape))\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                outputs[batch_element_index, di, :] = decoder_output\n",
    "                decoder_input = torch.tensor([target_tensor[di]], device=device).unsqueeze(1)\n",
    "                if debug == True: \n",
    "                    print(\"AFTER_DECODER_STEP decoder_output.shape {}, decoder_input.shape {}, decoder_input {}\".format(decoder_output.shape, decoder_input.shape, decoder_input))\n",
    "                if decoder_input[0][0] == 0:\n",
    "                    break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (model, iterator):\n",
    "    epoch_loss = 0\n",
    "    for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        with torch.no_grad():\n",
    "            input_seq = torch.tensor(batch[0]).to(device)\n",
    "            target_tokens = torch.tensor(batch[1]).to(device)\n",
    "            output = model(input_seq, target_tokens)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            target_tokens = target_tokens.view(-1)\n",
    "#             print(\"AFTER RESHAPE PREDICTED_OUTPUT {}, REAL_OUTPUT {}\".format(output.shape, target_tokens.shape))\n",
    "            loss = criterion(output, target_tokens)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer,epoch_number, debug = False):\n",
    "   # Put the model in training mode!\n",
    "    model.train()\n",
    "    losses_list = []\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(iterator), total=len(iterator), desc='Epoch {}'.format(epoch_number + 1))\n",
    "    for idx, batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_seq = torch.tensor(batch[0]).to(device)\n",
    "        target_tokens = torch.tensor(batch[1]).to(device)\n",
    "#         print(len(batch), batch[0][0],batch[1][0])\n",
    "        output = model(input_seq, target_tokens)\n",
    "        if debug == True:\n",
    "            print(\"PREDICTED_OUTPUT {}, REAL_OUTPUT {}\".format(output.shape, target_tokens.shape))\n",
    "#         output = output.view(-1)\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        target_tokens = target_tokens.view(-1)\n",
    "        if debug == True:\n",
    "            print(\"AFTER RESHAPE PREDICTED_OUTPUT {}, REAL_OUTPUT {}\".format(output.shape, target_tokens.shape))\n",
    "        loss = criterion(output, target_tokens)\n",
    "        losses_list.append(float(loss))\n",
    "        progress_bar.set_postfix(train_loss = np.mean(losses_list[-100:]))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "#         break\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MY VERSIOB\n",
    "pad_idx = tokenizer.vocab().index(\"<PAD>\")\n",
    "eos_idx = tokenizer.vocab().index(\"<EOS>\")\n",
    "sos_idx = tokenizer.vocab().index(\"<BOS>\")\n",
    "# Size of embedding_dim should match the dim of pre-trained word embeddings!\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "vocab_size = len(tokenizer.vocab())\n",
    "model = My_Seq2Seq(embedding_dim,\n",
    "                 hidden_dim, \n",
    "                 vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=1.0e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934bf3ac569d48c19da1c4beb75fa186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=1286.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "validation_losses =[]\n",
    "train_losses = []\n",
    "\n",
    "N_EPOCHS = 20\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if min(train_losses) == train_loss and len(train_losses) > 1:\n",
    "        torch.save(model.state_dict, \"best_seq2seq_no_att_no_early_stop\")\n",
    "        torch.save(optimizer.state_dict, \"best_Adam_state_dict_no_att_no_early_stop\")\n",
    "    \n",
    "    torch.save(model.state_dict, \"last_seq2seq_no_att_no_early_stop\")\n",
    "    torch.save(optimizer.state_dict, \"Adam_state_dict_no_att_no_early_stop\")\n",
    "    \n",
    "#     #early stopping\n",
    "#     test_loss = evaluate(model, validation_loader)\n",
    "#     validation_losses.append(test_loss)\n",
    "    \n",
    "#     if len(validation_losses) > 1 and validation_losses[epoch] > validation_losses[epoch-1]:\n",
    "#         print(\"stop\")\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
