{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer, clip=1.0):\n",
    "   # Put the model in training mode!\n",
    "   model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        \n",
    "        input_sequence = batch.input_sequence\n",
    "        output_sequence = batch.output_sequence\n",
    "        \n",
    "        target_tokens = output_sequence[0]\n",
    "        \n",
    "        # zero out the gradient for the current batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run the batch through our model\n",
    "        output = model(input_sequence, output_sequence)\n",
    "        \n",
    "        # Throw it through our loss function\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        target_tokens = target_tokens[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, target_tokens)\n",
    "        \n",
    "        # Perform back-prop and calculate the gradient of our loss function\n",
    "        loss.backward()\n",
    "          \n",
    "        # Clip the gradient if necessary.          \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx, teacher_forcing_ratio=0.5):\n",
    "        super(seq2seq, self).__init__()\n",
    "        \n",
    "        # Embedding layer shared by encoder and decoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Encoder network\n",
    "        self.encoder = Encoder(hidden_size, \n",
    "                               embedding_size, \n",
    "                               self.embedding,\n",
    "                              num_layers=2,\n",
    "                              dropout=0.5)\n",
    "        \n",
    "        # Decoder network        \n",
    "        self.decoder = Decoder(self.embedding,\n",
    "                               embedding_size,\n",
    "                              hidden_size,\n",
    "                              vocab_size,\n",
    "                              n_layers=2,\n",
    "                              dropout=0.5)\n",
    "        \n",
    "        \n",
    "        # Indices of special tokens and hardware device \n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, input_sequence):\n",
    "        return (input_sequence != self.pad_idx).permute(1, 0)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_sequence, output_sequence, teacher_forcing_ratio=0.5):\n",
    "      \n",
    "        # Unpack input_sequence tuple\n",
    "        input_tokens = input_sequence[0]\n",
    "        input_lengths = input_sequence[1]\n",
    "      \n",
    "        # Unpack output_tokens, or create an empty tensor for text generation\n",
    "        if output_sequence is None:\n",
    "            inference = True\n",
    "            output_tokens = torch.zeros((100, input_tokens.shape[1])).long().fill_(self.sos_idx).to(self.device)\n",
    "        else:\n",
    "            inference = False\n",
    "            output_tokens = output_sequence[0]\n",
    "vocab_size = self.decoder.output_size\n",
    "        \n",
    "        batch_size = len(input_lengths)\n",
    "        max_seq_len = len(output_tokens)\n",
    "        \n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_seq_len, batch_size, vocab_size).to(self.device)\n",
    "        \n",
    "        \n",
    "        # Pass through the first half of the network\n",
    "        encoder_outputs, hidden = self.encoder(input_tokens, input_lengths)\n",
    "        \n",
    "        # Ensure dim of hidden_state can be fed into Decoder\n",
    "        hidden =  hidden[:self.decoder.n_layers]\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = output_tokens[0,:]\n",
    "        \n",
    "        # Create mask\n",
    "        mask = self.create_mask(input_tokens)\n",
    "        \n",
    "        \n",
    "        # Step through the length of the output sequence one token at a time\n",
    "        # Teacher forcing is used to assist training\n",
    "        for t in range(1, max_seq_len):\n",
    "            output = output.unsqueeze(0)\n",
    "            \n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs, mask)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (output_tokens[t] if teacher_force else top1)\n",
    "            \n",
    "            # If we're in inference mode, keep generating until we produce an\n",
    "            # <eos> token\n",
    "            if inference and output.item() == self.eos_idx:\n",
    "                return outputs[:t]\n",
    "return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  \n",
    "    def __init__(self, hidden_size, embedding_size,\n",
    "                 embedding, num_layers=2, dropout=0.0):\n",
    "      \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Basic network params\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Embedding layer that will be shared with Decoder\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        # Bidirectional GRU\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          dropout=dropout,\n",
    "                          bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_sequence, input_lengths):\n",
    "        \n",
    "        # Convert input_sequence to word embeddings\n",
    "        word_embeddings = self.embedding(input_sequence)\n",
    "        \n",
    "        \n",
    "        # Pack the sequence of embeddings\n",
    "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(word_embeddings, input_lengths)\n",
    "        \n",
    "        # Run the packed embeddings through the GRU, and then unpack the sequences\n",
    "        outputs, hidden = self.gru(packed_embeddings)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        \n",
    "        # The ouput of a GRU has shape (seq_len, batch, hidden_size * num_directions)\n",
    "        # Because the Encoder is bidirectional, combine the results from the \n",
    "        # forward and reversed sequence by simply adding them together.\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "      \n",
    "    def dot_score(self, hidden_state, encoder_states):\n",
    "        return torch.sum(hidden_state * encoder_states, dim=2)\n",
    "    \n",
    "            \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "       \n",
    "        attn_scores = self.dot_score(hidden, encoder_outputs)\n",
    "# Transpose max_length and batch_size dimensions\n",
    "        attn_scores = attn_scores.t()\n",
    "# Apply mask so network does not attend <pad> tokens        \n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "# Return softmax over attention scores      \n",
    "return F.softmax(attn_scores, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding, embedding_size,\n",
    "                 hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Basic network params\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "self.embedding = embedding\n",
    "                \n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, n_layers, \n",
    "                          dropout=dropout)\n",
    "        \n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "self.attn = Attention(hidden_size)\n",
    "        \n",
    "    def forward(self, current_token, hidden_state, encoder_outputs, mask):\n",
    "      \n",
    "        # convert current_token to word_embedding\n",
    "        embedded = self.embedding(input_step)\n",
    "        \n",
    "        # Pass through GRU\n",
    "        rnn_output, hidden_state = self.gru(embedded, hidden_state)\n",
    "        \n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.attn(rnn_output, encoder_outputs, mask)\n",
    "        \n",
    "        # Calculate context vector\n",
    "        context = attentionn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        \n",
    "        # Concatenate  context vector and GRU output\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        \n",
    "        # Pass concat_output to final output layer\n",
    "        output = self.out(concat_output)\n",
    "        \n",
    "        # Return output and final hidden state\n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
