{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_QG_RNN_features.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK3D5lJ8b8Lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# References: https://medium.com/@adam.wearne/seq2seq-with-pytorch-46dc00ff5164,\n",
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibq9hTV8wKrZ",
        "colab_type": "text"
      },
      "source": [
        "## **Load data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udhfTZQy7zo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aetgAeC38cvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ls \"/content/drive/My Drive\"\n",
        "!cp \"/content/drive/My Drive/dataset/squad_train.csv\" \"squad_train.csv\"\n",
        "!cp \"/content/drive/My Drive/dataset/squad_dev.csv\" \"squad_dev.csv\"\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbATRiwD8sGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make test set & lexical features\n",
        "dev = pd.read_csv('squad_dev.csv')\n",
        "dev['LEX'] = ''\n",
        "for idx, pos, ner, case in dev[['POS', 'NER', 'case']].itertuples():\n",
        "  lex = [i + '_' + j + '_' + z for i, j, z in zip(pos.split(), ner.split(), case.split())]\n",
        "  dev['LEX'][idx] = ' '.join(lex)\n",
        "dev = dev[['context', 'question', 'BIO', 'LEX']]\n",
        "val, test = train_test_split(dev, test_size=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bmn8vj_A3K7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val.to_csv('squad_val.csv', index=False)\n",
        "test.to_csv('squad_test.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI-c3GL36pJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = pd.read_csv('squad_train.csv')\n",
        "train_file['LEX'] = ''\n",
        "for idx, pos, ner, case in train_file[['POS', 'NER', 'case']].itertuples():\n",
        "  lex = [i + '_' + j + '_' + z for i, j, z in zip(pos.split(), ner.split(), case.split())]\n",
        "  train_file['LEX'][idx] = ' '.join(lex)\n",
        "train_file = train_file[['context', 'question', 'BIO', 'LEX']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XeI8ONS7CkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file.to_csv('squad_train.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0ttdwMjdOdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Field object\n",
        "tokenize = lambda x: x.split()\n",
        "TEXT = data.Field(tokenize=tokenize, lower=False, include_lengths = True, \n",
        "                  init_token = '<sos>', eos_token = '<eos>')\n",
        "LEX = data.Field(tokenize=tokenize, lower=False, init_token = '<sos>', eos_token = '<sos>')\n",
        "BIO = data.Field(tokenize=tokenize, lower=False, init_token = '<sos>', eos_token = '<sos>')\n",
        "\n",
        "# Specify Fields in our dataset\n",
        "fields = [('context', TEXT), ('question', TEXT), ('bio', BIO), ('lex', LEX)]\n",
        "\n",
        "# Build the dataset\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(path = '',\n",
        "                                                    train='squad_train.csv',\n",
        "                                                    validation='squad_val.csv', \n",
        "                                                    test='squad_test.csv', \n",
        "                                                    fields = fields,\n",
        "                                                    format='csv', \n",
        "                                                    skip_header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azf6dujVBCMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nsdr2JeBI8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBgiHqtNs2mA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build vocabulary\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "MIN_COUNT = 3\n",
        "MAX_SEQUENCE_LENGTH = 15\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size=MAX_VOCAB_SIZE,\n",
        "                 min_freq=MIN_COUNT,\n",
        "                 vectors='glove.6B.300d',\n",
        "                 unk_init=torch.Tensor.normal_)\n",
        "\n",
        "BIO.build_vocab(train_data)\n",
        "LEX.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7-pKpDYBWkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(f\"Unique tokens in vocabulary: {len(TEXT.vocab)}\")\n",
        "print('bio', len(BIO.vocab))\n",
        "print('lex', len(LEX.vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsbihyTdApSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEX.vocab.stoi['<pad>']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqoPTaroAqNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BIO.vocab.stoi['<pad>']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGsSwKBWwIND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a set of iterators for each split\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x:len(x.context),\n",
        "     device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFR2LZ21x6B9",
        "colab_type": "text"
      },
      "source": [
        "## **Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejBnlQAYxxS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  \n",
        "    def __init__(self, hidden_size, embedding_size,\n",
        "                 embedding, answer_embedding, lexical_embedding, n_layers, dropout):\n",
        "      \n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        # Basic network params\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # Embedding layer that will be shared with Decoder\n",
        "        self.embedding = embedding\n",
        "        self.answer_embedding = answer_embedding\n",
        "        self.lexical_embedding = lexical_embedding\n",
        "        \n",
        "        # Bidirectional GRU\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size,\n",
        "                          num_layers=n_layers,\n",
        "                          dropout=dropout,\n",
        "                          bidirectional=True)\n",
        "        \n",
        "    def forward(self, input_sequence, input_lengths, answer_sequence, lexical_sequence):\n",
        "        \n",
        "        # Convert input_sequence to word embeddings\n",
        "        word_embeddings = self.embedding(input_sequence)\n",
        "        answer_embeddings = self.answer_embedding(answer_sequence)\n",
        "        lexical_embeddings = self.lexical_embedding(lexical_sequence)\n",
        "        final_embeddings = word_embeddings+answer_embeddings+lexical_embeddings\n",
        "        \n",
        "        # Pack the sequence of embeddings\n",
        "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(final_embeddings, input_lengths)\n",
        "        \n",
        "        # Run the packed embeddings through the GRU, and then unpack the sequences\n",
        "        outputs, hidden = self.gru(packed_embeddings)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        \n",
        "        \n",
        "        # The ouput of a GRU has shape (seq_len, batch, hidden_size * num_directions)\n",
        "        # Because the Encoder is bidirectional, combine the results from the \n",
        "        # forward and reversed sequence by simply adding them together.\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "\n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rn-Kfz_7Z0X",
        "colab_type": "text"
      },
      "source": [
        "## **Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvbzSoHx7ZIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "    def dot_score(self, hidden_state, encoder_states):\n",
        "\n",
        "        return torch.sum(hidden_state * encoder_states, dim=2)\n",
        "    \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        attn_scores = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_scores = attn_scores.t()\n",
        "        \n",
        "        # Apply mask so network does not attend <pad> tokens        \n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        # Return softmax over attention scores      \n",
        "        return F.softmax(attn_scores, dim=1).unsqueeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGyvb0nZZXfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding, embedding_size,\n",
        "                 hidden_size, output_size, n_layers, dropout):\n",
        "        \n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        # Basic network params\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.embedding = embedding\n",
        "                \n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, n_layers, \n",
        "                          dropout=dropout)\n",
        "        \n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.attn = Attention(hidden_size)\n",
        "        \n",
        "    def forward(self, current_token, hidden_state, encoder_outputs, mask):\n",
        "      \n",
        "        # convert current_token to word_embedding\n",
        "        embedded = self.embedding(current_token)\n",
        "        \n",
        "        # Pass through GRU\n",
        "        rnn_output, hidden_state = self.gru(embedded, hidden_state)\n",
        "        \n",
        "        # Calculate attention weights\n",
        "        attention_weights = self.attn(rnn_output, encoder_outputs, mask)\n",
        "        \n",
        "        # Calculate context vector\n",
        "        context = attention_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        \n",
        "        # Concatenate  context vector and GRU output\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        \n",
        "        # Pass concat_output to final output layer\n",
        "        output = self.out(concat_output)\n",
        "        \n",
        "        # Return output and final hidden state\n",
        "        return output, hidden_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptOTxmouakIb",
        "colab_type": "text"
      },
      "source": [
        "## **Seq2Seq**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zGxEHwdaGtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class seq2seq(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, vocab_size, \n",
        "                 device, pad_idx, eos_idx, sos_idx, teacher_forcing_ratio=0.5):\n",
        "        super(seq2seq, self).__init__()\n",
        "        \n",
        "        # Embedding layer shared by encoder and decoder\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.answer_embedding = nn.Embedding(6, embedding_size, padding_idx=1)\n",
        "        self.lexical_embedding = nn.Embedding(452, embedding_size, padding_idx=1)\n",
        "        \n",
        "        # Encoder network\n",
        "        self.encoder = Encoder(hidden_size, \n",
        "                               embedding_size, \n",
        "                               self.embedding,\n",
        "                               self.answer_embedding,\n",
        "                               self.lexical_embedding,\n",
        "                               n_layers=2,\n",
        "                               dropout=0.5)\n",
        "        \n",
        "        # Decoder network        \n",
        "        self.decoder = Decoder(self.embedding,\n",
        "                               embedding_size,\n",
        "                               hidden_size,\n",
        "                               vocab_size,\n",
        "                               n_layers=2,\n",
        "                               dropout=0.5)\n",
        "        \n",
        "        \n",
        "        # Indices of special tokens and hardware device \n",
        "        self.pad_idx = pad_idx\n",
        "        self.eos_idx = eos_idx\n",
        "        self.sos_idx = sos_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, input_sequence):\n",
        "\n",
        "        return (input_sequence != self.pad_idx).permute(1, 0)\n",
        "        \n",
        "    def forward(self, input_sequence, answer_sequence, lexical_sequence, output_sequence, teacher_forcing_ratio):\n",
        "      \n",
        "        # Unpack input_sequence tuple\n",
        "        input_tokens = input_sequence[0]\n",
        "        input_lengths = input_sequence[1]\n",
        "      \n",
        "        # Unpack output_tokens, or create an empty tensor for text generation\n",
        "        if output_sequence is None:\n",
        "            inference = True\n",
        "            output_tokens = torch.zeros((100, input_tokens.shape[1])).long().fill_(self.sos_idx).to(self.device)\n",
        "        else:\n",
        "            inference = False\n",
        "            output_tokens = output_sequence[0]\n",
        "        \n",
        "        vocab_size = self.decoder.output_size\n",
        "        \n",
        "        batch_size = len(input_lengths)\n",
        "        max_seq_len = len(output_tokens)\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_seq_len, batch_size, vocab_size).to(self.device)\n",
        "        \n",
        "        # Pass through the first half of the network\n",
        "        encoder_outputs, hidden = self.encoder(input_tokens, input_lengths, answer_sequence, lexical_sequence)\n",
        "        \n",
        "        # Ensure dim of hidden_state can be fed into Decoder\n",
        "        hidden =  hidden[:self.decoder.n_layers]\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        output = output_tokens[0,:]\n",
        "        \n",
        "        # Create mask\n",
        "        mask = self.create_mask(input_tokens)\n",
        "        \n",
        "        # Step through the length of the output sequence one token at a time\n",
        "        # Teacher forcing is used to assist training\n",
        "        for t in range(1, max_seq_len):\n",
        "            output = output.unsqueeze(0)\n",
        "            \n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs, mask)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (output_tokens[t] if teacher_force else top1)\n",
        "            \n",
        "            # If we're in inference mode, keep generating until we produce an\n",
        "            # <eos> token\n",
        "            if inference and output.item() == self.eos_idx:\n",
        "                return outputs[:t]\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmk2T_7Ddr_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_idx = TEXT.vocab.stoi['<pad>']\n",
        "eos_idx = TEXT.vocab.stoi['<eos>']\n",
        "sos_idx = TEXT.vocab.stoi['<sos>']\n",
        "\n",
        "# Size of embedding_dim should match the dim of pre-trained word embeddings\n",
        "embedding_dim = 300\n",
        "hidden_dim = 512\n",
        "vocab_size = len(TEXT.vocab)\n",
        "model = seq2seq(embedding_dim,\n",
        "                 hidden_dim, \n",
        "                 vocab_size, \n",
        "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.requires_grad = False\n",
        "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], \n",
        "                       lr=1.0e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm8KaxIBfbrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, criterion, optimizer, clip):\n",
        "    # Put the model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
        "        \n",
        "        input_sequence = batch.context\n",
        "        answer_sequence = batch.bio\n",
        "        output_sequence = batch.question\n",
        "        lexical_sequence = batch.lex\n",
        "        \n",
        "        target_tokens = output_sequence[0]\n",
        "        \n",
        "        # zero out the gradient for the current batch\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Run the batch through our model\n",
        "        output = model(input_sequence, answer_sequence, lexical_sequence, output_sequence, 0.5)\n",
        "        \n",
        "        # Throw it through our loss function\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        target_tokens = target_tokens[1:].view(-1)\n",
        "        \n",
        "        loss = criterion(output, target_tokens)\n",
        "        \n",
        "        # Perform back-prop and calculate the gradient of our loss function\n",
        "        loss.backward()\n",
        "          \n",
        "        # Clip the gradient if necessary.          \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlLaZbByYbfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
        "\n",
        "            input_sequence = batch.context\n",
        "            answer_sequence = batch.bio\n",
        "            output_sequence = batch.question\n",
        "            lexical_sequence = batch.lex\n",
        "            \n",
        "            target_tokens = output_sequence[0]\n",
        "            \n",
        "            output = model(input_sequence, answer_sequence, lexical_sequence, output_sequence, 0)\n",
        "            \n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            target_tokens = target_tokens[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, target_tokens)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtpUO5ZGhDpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    train_loss = train(model, train_iterator, criterion, optimizer, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "    print('Epoch: ', epoch)\n",
        "    print('train loss: ', train_loss)\n",
        "    print('valid loss: ', valid_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aeybgqAcqy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhHyPaVQno1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_sentence(model, paragraph, answer_pos, lex_features):\n",
        "    model.eval()\n",
        "    \n",
        "    tokenized = ['<sos>'] + paragraph + ['<eos>']\n",
        "    numericalized = [TEXT.vocab.stoi[t] for t in tokenized] \n",
        "\n",
        "    tokenized_answer = ['<sos>'] + answer_pos + ['<eos>']\n",
        "    numericalized_answer = [BIO.vocab.stoi[t] for t in tokenized_answer] \n",
        "\n",
        "    tokenized_lex = ['<sos>'] + lex_features + ['<eos>']\n",
        "    numericalized_lex = [LEX.vocab.stoi[t] for t in tokenized_lex]\n",
        "    \n",
        "    paragraph_length = torch.LongTensor([len(numericalized)]).to(model.device) \n",
        "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(model.device) \n",
        " \n",
        "    answer_tensor = torch.LongTensor(numericalized_answer).unsqueeze(1).to(model.device) \n",
        "    lex_tensor = torch.LongTensor(numericalized_lex).unsqueeze(1).to(model.device)\n",
        "    \n",
        "    question_tensor_logits = model((tensor, paragraph_length), answer_tensor, lex_tensor, None, 0) \n",
        "    \n",
        "    question_tensor = torch.argmax(question_tensor_logits.squeeze(1), 1)\n",
        "    question = [TEXT.vocab.itos[t] for t in question_tensor]\n",
        " \n",
        "    # Start at the first index.  We don't need to return the <sos> token\n",
        "    question = question[1:]\n",
        "\n",
        "    return question, question_tensor_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynVtLi8zoUDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_idx = 1\n",
        "\n",
        "src = vars(train_data.examples[example_idx])['context']\n",
        "trg = vars(train_data.examples[example_idx])['question']\n",
        "ans = vars(train_data.examples[example_idx])['bio']\n",
        "lex = vars(train_data.examples[example_idx])['lex']\n",
        "\n",
        "print('src: ', ' '.join(src))\n",
        "print('trg: ', ' '.join(trg))\n",
        "\n",
        "question, logits = translate_sentence(model, src, ans, lex)\n",
        "print('predicted: ', \" \".join(question))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxIdhwBq-ikD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cKAMa_L5wx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_bleu(data, model):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = vars(datum)['context']\n",
        "        trg = vars(datum)['question']\n",
        "        ans = vars(datum)['bio']\n",
        "        lex = vars(datum)['lex']\n",
        "        \n",
        "        pred_trg, _ = translate_sentence(model, src, ans, lex)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        # print(pred_trg)\n",
        "        trgs.append(trg)\n",
        "        # print(trg)\n",
        "        \n",
        "    return corpus_bleu(pred_trgs, trgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlQc_znf6ATf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bleu_score = calculate_bleu(test_data, model)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAIOVs9DlC-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " for instance in list(tqdm._instances):\n",
        "    tqdm._decr_instances(instance)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}