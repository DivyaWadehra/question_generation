{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtokentome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/25/e2f9863b78e5aef61bc0475bfac39f56197103f767e6f2e957cc67b989f2/youtokentome-1.0.5.tar.gz (86kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 9.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Click>=7.0 (from youtokentome)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 3.8MB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: youtokentome\n",
      "  Running setup.py bdist_wheel for youtokentome ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/30/65/29/959c7ddc32cd220a9e633c5818802efe41a990c476220aed69\n",
      "Successfully built youtokentome\n",
      "Installing collected packages: Click, youtokentome\n",
      "Successfully installed Click-7.0 youtokentome-1.0.5\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install youtokentome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import youtokentome as yttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            context  \\\n",
       "0           0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1           1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2           2  Managed by her father, Mathew Knowles, the gro...   \n",
       "3           3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4           4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question  \n",
       "0           When did Beyonce start becoming popular?  \n",
       "1  What areas did Beyonce compete in when she was...  \n",
       "2  When did Beyonce leave Destiny's Child and bec...  \n",
       "3      In what city and state did Beyonce  grow up?   \n",
       "4         In which decade did Beyonce become famous?  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ИЗМЕНИТЬ ПУТЬ В ЗАВИСИМОСТИ ОТ ТОГО ГДЕ ЗАПУСКАЕМ КОД\n",
    "try:\n",
    "    df = pd.read_csv(\"ctx_quest.csv\")\n",
    "except:\n",
    "    df = pd.read_csv(\"/Users/lilyakhoang/input/question_generation/ctx_quest.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 86610 entries, 0 to 86820\n",
      "Data columns (total 3 columns):\n",
      "Unnamed: 0    86610 non-null int64\n",
      "context       86610 non-null object\n",
      "question      86610 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_nonan = df.dropna()\n",
    "df_nonan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # для \"обучения\" bpe модели нам нужно сохранить данные для обучения в отдельный файл\n",
    "# # где будут построчно храниться тексты\n",
    "# err = 0\n",
    "# try:\n",
    "#     f = open('/Users/lilyakhoang/input/for_bpe_ctx_quest.txt', 'w')\n",
    "#     bpe_model_address = '/Users/lilyakhoang/input/for_bpe_ctx_quest.txt'\n",
    "# except:\n",
    "#     f = open('for_bpe_ctx_quest.txt', 'w')\n",
    "#     bpe_model_address = 'for_bpe_ctx_quest.txt'\n",
    "# for que in df_nonan.context:\n",
    "#     try:\n",
    "#         f.write(que + '\\n')\n",
    "#     except:\n",
    "#         err += 1\n",
    "# for que in df_nonan.question:\n",
    "#     try:\n",
    "#         f.write(que + '\\n')\n",
    "#     except:\n",
    "#         err += 1\n",
    "# f.close()\n",
    "# print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем\n",
    "vocab_size = 16000\n",
    "model_path = 'bpe.model'\n",
    "\n",
    "# yttm.BPE.train(data='for_bpe_ctx_quest.txt', vocab_size=vocab_size, model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(model=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Убрал BOS и EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c424561ddb4d108c6c30d2290f695b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=339), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff14a3d9565d44e9b55185f2e47c45e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=339), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# давайте токенизируем наш датасет\n",
    "# токенизирую батчами, потому что так быстрее\n",
    "# также в начало добавляем токен bos (begin of sentence)\n",
    "\n",
    "tokenized_ctx = []\n",
    "tokenized_quest = []\n",
    "batch_size = 256\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(df_nonan.context) / batch_size))):\n",
    "    \n",
    "    tokenized_ctx.extend(tokenizer.encode(list(df_nonan.context[i_batch*batch_size:(i_batch+1)*batch_size])))\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(df_nonan.question) / batch_size))):\n",
    "\n",
    "    tokenized_quest.extend(tokenizer.encode(list(df_nonan.question[i_batch*batch_size:(i_batch+1)*batch_size])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример с переводом:\n",
    "1. что подаем на вход энкодеру: мама мыла раму\n",
    "1. что подаем на вход декодеру: bos mom was washing the frame\n",
    "1. наши таргеты (от чего считать лосс): mom was washing the frame eos\n",
    "\n",
    "То есть наши таргеты это почти тоже самое, что мы подаем на вход декодеру, но по сути это смещенная вправо последовательность. Мы обрубаем наши тексты до максимальной длины - 1 (не учитывая bos и eos теги), потому что в последовательности 2 и 3 мы добавим по одному тегу (bos и eos). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_sequence = 'мама мыла раму'.split()\n",
    "decoder_sequence = 'bos mom was washing the frame'.split()\n",
    "target_sequence = 'mom was washing the frame eos'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoder_sequence) == len(target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "По слову \"bos\" предсказываем слово \"mom\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"bos\" при условии того, что в памяти есть [] предсказываем слово \"mom\"\n",
      "\n",
      "По слову \"mom\" предсказываем слово \"was\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"mom\" при условии того, что в памяти есть ['bos'] предсказываем слово \"was\"\n",
      "\n",
      "По слову \"was\" предсказываем слово \"washing\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"was\" при условии того, что в памяти есть ['bos', 'mom'] предсказываем слово \"washing\"\n",
      "\n",
      "По слову \"washing\" предсказываем слово \"the\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"washing\" при условии того, что в памяти есть ['bos', 'mom', 'was'] предсказываем слово \"the\"\n",
      "\n",
      "По слову \"the\" предсказываем слово \"frame\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"the\" при условии того, что в памяти есть ['bos', 'mom', 'was', 'washing'] предсказываем слово \"frame\"\n",
      "\n",
      "По слову \"frame\" предсказываем слово \"eos\"\n",
      "Или с точки зрения rnn\n",
      "По слову \"frame\" при условии того, что в памяти есть ['bos', 'mom', 'was', 'washing', 'the'] предсказываем слово \"eos\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(decoder_sequence)):\n",
    "    print(f'По слову \"{decoder_sequence[n]}\" предсказываем слово \"{target_sequence[n]}\"')\n",
    "    print('Или с точки зрения rnn')\n",
    "    print(f'По слову \"{decoder_sequence[n]}\" при условии того, что в памяти есть {decoder_sequence[:n]} предсказываем слово \"{target_sequence[n]}\"')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошо было бы переписать loader с такими последовательностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(torch.utils.data.Dataset):\n",
    "    def __init__(self, context_list, questions_list, context_len, questions_len, pad_index, eos_index):\n",
    "        self.context_list = context_list\n",
    "        self.questions_list = questions_list\n",
    "        \n",
    "        self.context_len = context_len\n",
    "        self.questions_len = questions_len\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.eos_index = eos_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.context_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        encoder_sequence = self.context_list[index][:self.context_len]\n",
    "        decoder_sequence = self.questions_list[index][:self.questions_len]\n",
    "        \n",
    "        target_sequence = decoder_sequence[:] + [self.eos_index]\n",
    "        decoder_sequence = decoder_sequence[:]\n",
    "        \n",
    "        encoder_pads = [self.pad_index] * (self.context_len - len(encoder_sequence))\n",
    "        decoder_pads = [self.pad_index] * (self.questions_len - len(decoder_sequence))\n",
    "        target_pads = [self.pad_index] * (self.questions_len - len(decoder_sequence))\n",
    "        \n",
    "        encoder_sequence = torch.tensor(encoder_sequence + encoder_pads).long()\n",
    "        decoder_sequence = torch.tensor(decoder_sequence + decoder_pads).long()\n",
    "        target_sequence = torch.tensor(target_sequence + target_pads).long()\n",
    "        \n",
    "        return encoder_sequence, decoder_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "context_len = 80\n",
    "quest_len = 20\n",
    "\n",
    "pad_index = 0\n",
    "eos_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start_index = int(len(tokenized_ctx) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77949, 8661)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = WordData(context_list=tokenized_ctx[:-validation_start_index],\n",
    "                         questions_list = tokenized_quest[:-validation_start_index],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "validation_dataset = WordData(context_list=tokenized_ctx[-validation_start_index:],\n",
    "                              questions_list = tokenized_quest[-validation_start_index:],\n",
    "                         context_len=context_len, questions_len = quest_len, pad_index=pad_index, eos_index=eos_index)\n",
    "\n",
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3297,  1394,  1304, 13539, 12243,  1838,  4502,  4113,  8059, 13895,\n",
      "          100,    90,    65,   197,     9,  1444,   112,    76,  1348,     5,\n",
      "           42,    73,    62,    51,  2250,  1398,    47,  4690,  2117,  2811,\n",
      "         6982,  2156,  8762,  1352,  1306,  1959,  2339,  2087,  3046,  7582,\n",
      "         2087,  2083,  7536,  1312,  1875, 11109,  1365,  2117,  1312,  5267,\n",
      "         1305,  4274, 13461, 15846,  2402,  4829,  1305,  2734, 10861,  1312,\n",
      "         1317,  9041, 12046,  1363,  1285, 12745,  1312,  7309,  1319, 13738,\n",
      "         1305,  1287,  2535,  7915,  1363,  2018,  8292,  1300,  9914,  8155]) tensor([1800, 1503, 7635, 2294, 4596, 2454,   36,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]) tensor([1800, 1503, 7635, 2294, 4596, 2454,   36,    3,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "for encoder_sequence, decoder_sequence, target_sequence in train_loader:\n",
    "    print(encoder_sequence[0], decoder_sequence[0], target_sequence[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 21])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "эмбеддинг энкодер последовательности\n",
    "encoder_embed = self.embedding_layer(encoder_sequence)\n",
    "# эмбеддинг декодер последовательности\n",
    "decoder_embed = self.embedding_layer(decoder_sequence)\n",
    "\n",
    "# пропускаем энкодер последовательность через энкодер, получаем хайден и память\n",
    "encoder_hidden, encoder_mem = self.encoder(encoder_embed)\n",
    "\n",
    "# добавляем последний (с точки зрения времени) хайден в начало эмбеддингов декодер последовательности\n",
    "# то есть по сути первый токен (эмбеддинг токена) декодера - это последний хайден из энкодера\n",
    "decoder_embed = torch.cat([encoder_hidden[:, -1, :].unsqueeze(1), decoder_embed], dim=1)\n",
    "\n",
    "# пропускаем декодер последовательность через декодер, получаем хайден и память\n",
    "# в качестве инициализирующей памяти вставляем память с энкодера\n",
    "decoder_hidden, _ = self.decoder(decoder_embed, encoder_mem)\n",
    "\n",
    "# предсказываем слова\n",
    "prediction = self.head(decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSequence2Sequence(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size=16000, embedding_size=256,\n",
    "                 hidden_size=256, num_layers=2, dropout=0.3, padding_idx=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embedding_size, padding_idx)\n",
    "        \n",
    "        self.encoder = torch.nn.GRU(embedding_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, 80)\n",
    "        \n",
    "        self.decoder = torch.nn.GRU(hidden_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.head = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, encoder_sequence, decoder_sequence):\n",
    "        \n",
    "        encoder_embed = self.embedding_layer(encoder_sequence)\n",
    "        decoder_embed = self.embedding_layer(decoder_sequence)\n",
    "        print(\"encoder_embed\",encoder_embed.shape)\n",
    "        encoder_hidden, encoder_mem = self.encoder(encoder_embed)\n",
    "        print(\"encoder_hidden\",encoder_hidden.shape)\n",
    "        decoder_embed = torch.cat([encoder_hidden[:, -1, :].unsqueeze(1), decoder_embed], dim=1)\n",
    "        print(\"decoder_embed\",decoder_embed.shape)\n",
    "        decoder_hidden, _ = self.decoder(decoder_embed, encoder_mem)\n",
    "        print(\"decoder_hidden\",decoder_hidden.shape)\n",
    "        prediction = self.head(decoder_hidden)\n",
    "        print(\"prediction\",prediction.shape)\n",
    "        print(\"====\")\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSequence2Sequence_attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size=16000, embedding_size=256,\n",
    "                 hidden_size=256, num_layers=2, dropout=0.3, padding_idx=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embedding_size, padding_idx)\n",
    "        \n",
    "        self.encoder = torch.nn.GRU(embedding_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.attn = nn.Linear(self.hidden_size, 80)\n",
    "        \n",
    "        self.decoder = torch.nn.GRU(hidden_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.head = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, encoder_sequence, decoder_sequence, debug = True):\n",
    "        \n",
    "        encoder_embed = self.embedding_layer(encoder_sequence)\n",
    "        decoder_embed = self.embedding_layer(decoder_sequence)\n",
    "        if debug == True: print(\"encoder_embed\",encoder_embed.shape)\n",
    "        encoder_hidden, encoder_mem = self.encoder(encoder_embed)\n",
    "        if debug == True: print(\"encoder_hidden\",encoder_hidden.shape)\n",
    "        attn_weights = torch.softmax(self.attn(encoder_hidden), dim=1)\n",
    "        if debug == True: print(\"attn_weights.shape {}, encoder_hidden.shape {}\".format(attn_weights.shape, encoder_hidden.shape))\n",
    "        attn_applied = torch.bmm(attn_weights,encoder_hidden)\n",
    "        decoder_embed = torch.cat([attn_applied[:, -1, :].unsqueeze(1), decoder_embed], dim=1)\n",
    "        if debug == True: print(\"decoder_embed\",decoder_embed.shape)\n",
    "        decoder_hidden, _ = self.decoder(decoder_embed, encoder_mem)\n",
    "        if debug == True: print(\"decoder_hidden\",decoder_hidden.shape)\n",
    "        prediction = self.head(decoder_hidden)\n",
    "        if debug == True: \n",
    "            print(\"prediction\",prediction.shape)\n",
    "            print(\"====\")\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestamp in x.size(1)\n",
    "    current_step = decoder_embed[:, timestamp, :]\n",
    "    current_decoder_rnn_hidden, decoder_mem = self.decoder(current_step, decoder_mem)\n",
    "    current_decoder_attention = self.attn(current_decoder_rnn_hidden, current_decoder_rnn_hidden)\n",
    "    ord_prediction = self.head(torch.cat([current_decoder_attention, current_decoder_rnn_hidden]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ENCODER_WORDS = 2\n",
    "DECODER_WORDS = 3\n",
    "EMBEDDING_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2, 4]), torch.Size([32, 3, 4]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "encoder_hidden = torch.rand(BATCH_SIZE, ENCODER_WORDS, EMBEDDING_DIM)\n",
    "decoder_hidden = torch.rand(BATCH_SIZE, DECODER_WORDS, EMBEDDING_DIM)\n",
    "encoder_hidden.shape, decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 4]), torch.Size([32, 4, 2]))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden.shape, encoder_hidden.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = torch.bmm(decoder_hidden, encoder_hidden.transpose(1, 2))\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 2]), torch.Size([32, 2, 4]))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_distribution = torch.softmax(attention_scores, 2)\n",
    "attention_distribution.shape, encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 4])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_vectors = torch.bmm(attention_distribution, encoder_hidden)\n",
    "attention_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 4]), torch.Size([32, 3, 4]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden.shape, attention_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 8])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_with_attention = torch.cat([decoder_hidden, attention_vectors], dim=-1)\n",
    "decoder_with_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "аттеншин между текущим словом в декодере и encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_mem = encoder_mem\n",
    "for timestamp in x.size(1)\n",
    "current_step = decoder_embed[:, timestamp, :]\n",
    "current_decoder_rnn_hidden, decoder_mem = self.decoder(current_step, decoder_mem)\n",
    "current_decoder_attention = self.attn(current_decoder_rnn_hidden, current_decoder_rnn_hidden)\n",
    "word_prediction = self.head(torch.cat([current_decoder_attention, current_decoder_rnn_hidden]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSequence2Sequence_attention_cycle(torch.nn.Module):\n",
    "    def __init__(self, vocab_size=16000, embedding_size=256,\n",
    "                 hidden_size=256, num_layers=2, dropout=0.3, padding_idx=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embedding_size, padding_idx)\n",
    "        \n",
    "        self.encoder = torch.nn.GRU(embedding_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.decoder = torch.nn.GRU(hidden_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.head = torch.nn.Linear(hidden_size*2, vocab_size)\n",
    "        \n",
    "    def forward(self, encoder_sequence, decoder_sequence, debug = True):\n",
    "        \n",
    "        encoder_embed = self.embedding_layer(encoder_sequence)\n",
    "        encoder_hidden, encoder_mem = self.encoder(encoder_embed)\n",
    "        decoder_mem = encoder_mem#.transpose(0,1).transpose(1,2)\n",
    "        decoder_embed = self.embedding_layer(decoder_sequence)\n",
    "        decoder_embed = torch.cat([encoder_hidden[:, -1, :].unsqueeze(1), decoder_embed], dim=1)\n",
    "        word_predictions = torch.zeros(64,21,16000)\n",
    "        for timestamp in range(decoder_embed.size(1)):\n",
    "            current_step = decoder_embed[:, timestamp, :]\n",
    "            current_decoder_rnn_hidden, decoder_mem = self.decoder(current_step.unsqueeze(1), decoder_mem)\n",
    "            current_decoder_attention = self.attn(current_decoder_rnn_hidden)\n",
    "            print(\"current_decoder_attention\", current_decoder_attention.shape)\n",
    "            cat = torch.cat([current_decoder_attention, current_decoder_rnn_hidden], dim = -1)\n",
    "            word_prediction = self.head(cat)\n",
    "            word_predictions[:,timestamp,:] = word_prediction.squeeze(1)\n",
    "        \n",
    "        return word_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSequence2Sequence_attention_cycle(torch.nn.Module):\n",
    "    def __init__(self, vocab_size=16000, embedding_size=256,\n",
    "                 hidden_size=256, num_layers=2, dropout=0.3, padding_idx=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embedding_size, padding_idx)\n",
    "        \n",
    "        self.encoder = torch.nn.GRU(embedding_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.decoder = torch.nn.GRU(hidden_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.head = torch.nn.Linear(hidden_size*2, vocab_size)\n",
    "        \n",
    "    def forward(self, encoder_sequence, decoder_sequence, debug = True):\n",
    "        encoder_embed = self.embedding_layer(encoder_sequence)\n",
    "        encoder_hidden, encoder_mem = self.encoder(encoder_embed)\n",
    "        encoder_hidden, encoder_mem = torch.nn.utils.rnn.pad_packed_sequence(encoder_hidden, batch_first=True)\n",
    "        decoder_mem = encoder_mem#.transpose(0,1).transpose(1,2)\n",
    "        decoder_embed = self.embedding_layer(decoder_sequence)\n",
    "        decoder_embed = torch.cat([encoder_hidden[:, -1, :].unsqueeze(1), decoder_embed], dim=1)\n",
    "        word_predictions = torch.zeros(64,21,16000)\n",
    "        for timestamp in range(decoder_embed.size(1)):\n",
    "            current_step = decoder_embed[:, timestamp, :]\n",
    "            current_decoder_rnn_hidden, decoder_mem = self.decoder(current_step.unsqueeze(1), decoder_mem)\n",
    "            attention_scores = torch.bmm(current_decoder_rnn_hidden, encoder_hidden.transpose(1, 2))\n",
    "            attention_distribution = torch.softmax(attention_scores, 2)\n",
    "            current_decoder_attention = torch.bmm(attention_distribution, encoder_hidden)\n",
    "            cat = torch.cat([current_decoder_attention, current_decoder_rnn_hidden], dim = -1)\n",
    "            word_prediction = self.head(cat)\n",
    "            word_predictions[:,timestamp,:] = word_prediction.squeeze(1)\n",
    "        \n",
    "        return word_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_decoder_attention torch.Size([64, 1, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efdf89bf756406595c828cdda876883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1218), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 7,  7,  7,  ...,  7,  7,  7],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [35, 35, 35,  ..., 35, 35, 35],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]])\n",
      "tensor([[80, 80, 80,  ..., 80, 80, 80],\n",
      "        [80, 80, 80,  ..., 80, 80, 80],\n",
      "        [73, 73, 73,  ..., 73, 73, 73],\n",
      "        ...,\n",
      "        [80, 80, 80,  ..., 80, 80, 80],\n",
      "        [45, 45, 45,  ..., 45, 45, 45],\n",
      "        [80, 80, 80,  ..., 80, 80, 80]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "'lengths' argument should be a 1D CPU int64 tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-403-324f95fb60c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-343-2a883e464e51>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, criterion, optimizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtarget_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m#         print(\"model output\", output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-401-90ba86d5227d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_sequence, decoder_sequence, debug)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mencoder_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first)\u001b[0m\n\u001b[1;32m    146\u001b[0m                       category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[1;32m    147\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 'lengths' argument should be a 1D CPU int64 tensor"
     ]
    }
   ],
   "source": [
    "model = SimpleSequence2Sequence_attention_cycle()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "train(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer):\n",
    "    \n",
    "   # Put the model in training mode!\n",
    "    model.train()\n",
    "\n",
    "    losses_list = []\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(iterator, total=len(iterator))\n",
    "    \n",
    "    for encoder_sequence, decoder_sequence, target_sequence in progress_bar:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        encoder_sequence = encoder_sequence.to(device)\n",
    "        decoder_sequence = decoder_sequence.to(device)\n",
    "        target_sequence = target_sequence.to(device)\n",
    "\n",
    "        output = model(encoder_sequence, decoder_sequence)\n",
    "#         print(\"model output\", output.shape)\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        \n",
    "        target_sequence = target_sequence.view(-1)\n",
    "#         print(\"to be sent to criterion\", output.shape, target_sequence.shape)\n",
    "        loss = criterion(output, target_sequence)\n",
    "        losses_list.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix(train_loss = np.mean(losses_list[-500:]))\n",
    "    return losses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = tokenizer.vocab().index(\"<PAD>\")\n",
    "eos_idx = tokenizer.vocab().index(\"<EOS>\")\n",
    "sos_idx = tokenizer.vocab().index(\"<BOS>\")\n",
    "# Size of embedding_dim should match the dim of pre-trained word embeddings!\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "vocab_size = len(tokenizer.vocab())\n",
    "model = My_seq2seq_attention(embedding_dim,\n",
    "                 hidden_dim, \n",
    "                 vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=1.0e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "train_losses = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, epoch)\n",
    "#     print (train_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    if min(train_losses) == train_loss and len(train_losses) > 1:\n",
    "        torch.save(model.state_dict, \"best_seq2seq_attention\")\n",
    "        torch.save(optimizer.state_dict, \"best_Adam_state_dict_attention\")\n",
    "    \n",
    "    torch.save(model.state_dict, \"last_seq2seq_attention\")\n",
    "    torch.save(optimizer.state_dict, \"Adam_state_dict_attention\")\n",
    "    \n",
    "    #early stopping\n",
    "    validation_losses = []\n",
    "    test_loss = evaluate(model, validation_loader)\n",
    "    validation_losses.append(test_loss)\n",
    "    \n",
    "    if len(validation_losses) > 1 and validation_losses[epoch] > validation_losses[epoch-1]:\n",
    "        print(\"stop\")\n",
    "        break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN_inside_class(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, embedding):#input_size\n",
    "        super(EncoderRNN_inside_class, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, input, hidden, debug = False):\n",
    "        if debug == True: \n",
    "            print(\"====ENCODING_FORWARD====\")\n",
    "            print(\"input.shape\", input.shape)\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        if debug == True: \n",
    "            print(\"embedded/output.shape\",embedded.shape,\"hidden.shape\", hidden.shape  )\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# исправленный\n",
    "# необязательно задавать хайден, читай доку\n",
    "class EncoderRNN_inside_class(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, embedding):#input_size\n",
    "        super(EncoderRNN_inside_class, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first = True)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "\n",
    "        embedded = self.embedding(sequence)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder_inside_class(nn.Module):\n",
    "    def __init__(self, embedding, embedding_size,\n",
    "                 hidden_size, output_size):\n",
    "        super(AttentionDecoder_inside_class, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, 80)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, debug = False):\n",
    "        embedded = self.embedding(input)\n",
    "        if debug == True:\n",
    "            print(\"embedded[0].shape {}, hidden.shape {}\".format(embedded[0].shape, hidden.shape))\n",
    "        attn_weights = torch.softmax(self.attn(torch.cat((embedded[0], hidden), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        if debug == True: \n",
    "            print(\"output.shape {}, hidden.shape {}\".format(output.shape,hidden.shape ))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        hidden = hidden.squeeze(1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_seq2seq_attention(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx):\n",
    "        super(My_seq2seq_attention, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Encoder network\n",
    "        self.encoder = EncoderRNN_inside_class(hidden_size, \n",
    "                               embedding_size, \n",
    "                               self.embedding)\n",
    "        \n",
    "        # Decoder network        \n",
    "        self.decoder = AttentionDecoder_inside_class(self.embedding,\n",
    "                               embedding_size,\n",
    "                              hidden_size,\n",
    "                              vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, input_sequence, output_sequence, debug = False):\n",
    "        \n",
    "\n",
    "        encoder_output, encoder_hidden = self.encoder(input_sequence)\n",
    "        batch_size = len(input_sequence)\n",
    "        outputs = torch.zeros(batch_size, 20, self.vocab_size).to(self.device)\n",
    "        \n",
    "        for batch_element_index in range(batch_size):\n",
    "            \n",
    "            target_tensor = output_sequence[batch_element_index, :]\n",
    "                \n",
    "            decoder_input = torch.tensor([[self.sos_idx]], device=device)\n",
    "\n",
    "            decoder_hidden = encoder_hidden[:,batch_element_index,:]\n",
    "\n",
    "            encoder_output_in_current_index = encoder_output[batch_element_index,:,:]\n",
    "\n",
    "            for di in range(20):\n",
    "                \n",
    "                decoder_output, decoder_hidden = self.decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_output_in_current_index)\n",
    "                \n",
    "                outputs[batch_element_index,di,:] = decoder_output\n",
    "                \n",
    "                if target_tensor[di] != self.pad_idx:\n",
    "                    decoder_input = torch.tensor([target_tensor[di]], device=device).unsqueeze(1)\n",
    "                else:\n",
    "                    break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer,epoch_number, debug = False):\n",
    "   # Put the model in training mode!\n",
    "    model.train()\n",
    "\n",
    "    losses_list = []\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(iterator), total=len(iterator), desc='Epoch {}'.format(epoch_number + 1))\n",
    "    for idx, batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_seq = torch.tensor(batch[0]).to(device)\n",
    "        target_tokens = torch.tensor(batch[1]).to(device)\n",
    "\n",
    "        output = model(input_seq, target_tokens)\n",
    "\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        target_tokens = target_tokens.view(-1)\n",
    "        loss = criterion(output, target_tokens)\n",
    "        losses_list.append(float(loss))\n",
    "        progress_bar.set_postfix(train_loss = np.mean(losses_list[-100:]))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "#         break\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (model, iterator):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(iterator), total=len(iterator))\n",
    "    for idx, batch in progress_bar:\n",
    "        with torch.no_grad():\n",
    "            input_seq = torch.tensor(batch[0]).to(device)\n",
    "            target_tokens = torch.tensor(batch[1]).to(device)\n",
    "            output = model(input_seq, target_tokens)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            target_tokens = target_tokens.view(-1)\n",
    "            loss = criterion(output, target_tokens)\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(train_loss = np.mean(losses_list[-100:]))\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for idx, batch in tqdm(enumerate(validation_loader), total=len(validation_loader)):\n",
    "        print(\"0\", batch[0])\n",
    "        print(\"1\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = tokenizer.vocab().index(\"<PAD>\")\n",
    "eos_idx = tokenizer.vocab().index(\"<EOS>\")\n",
    "sos_idx = tokenizer.vocab().index(\"<BOS>\")\n",
    "# Size of embedding_dim should match the dim of pre-trained word embeddings!\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "vocab_size = len(tokenizer.vocab())\n",
    "model = My_seq2seq_attention(embedding_dim,\n",
    "                 hidden_dim, \n",
    "                 vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=1.0e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss = train(model, train_loader, criterion, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = train_loader\n",
    "epoch_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94161795ab2b4a81aa35f94fc08bf326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=1286.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "losses_list = []\n",
    "epoch_loss = 0\n",
    "progress_bar = tqdm(enumerate(iterator), total=len(iterator), desc='Epoch {}'.format(epoch_number + 1))\n",
    "for idx, batch in progress_bar:\n",
    "    optimizer.zero_grad()\n",
    "    input_seq = torch.tensor(batch[0]).to(device)\n",
    "    target_tokens = torch.tensor(batch[1]).to(device)\n",
    "\n",
    "    break\n",
    "    \n",
    "    output = model(input_seq, target_tokens)\n",
    "    output = output.view(-1, output.size(-1))\n",
    "    target_tokens = target_tokens.view(-1)\n",
    "    \n",
    "    loss = criterion(output, target_tokens)\n",
    "    losses_list.append(loss.item())\n",
    "    progress_bar.set_postfix(train_loss = np.mean(losses_list[-100:]))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in iterator:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  3297,  1394,  ...,  8292,  1300,  9914],\n",
       "        [    2,  3297,  1394,  ...,  8292,  1300,  9914],\n",
       "        [    2,  2570,  4376,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,  3917, 10861,  ...,  3711,  1487,  9139],\n",
       "        [    2,  3308,  1585,  ...,     0,     0,     0],\n",
       "        [    2,  2733,  1462,  ...,  2102,  2904,  1287]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2, 1800, 1503,  ...,    0,    0,    0],\n",
       "        [   2, 1411, 2461,  ...,    0,    0,    0],\n",
       "        [   2, 1800, 1503,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   2, 1615, 2459,  ...,    0,    0,    0],\n",
       "        [   2, 1411, 4162,  ...,    0,    0,    0],\n",
       "        [   2, 1723, 5054,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_sequence, output_sequence in iterator:\n",
    "    input_sequence = input_sequence.to(device)\n",
    "    output_sequence = output_sequence.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 80]), torch.Size([64, 20]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence.shape, output_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output, encoder_hidden = model.encoder(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 300])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSequence2Sequence(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size=16000, embedding_size=256,\n",
    "                 hidden_size=256, num_layers=2, dropout=0.3, padding_idx=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embedding_size, padding_idx)\n",
    "        \n",
    "        self.encoder = torch.nn.GRU(embedding_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.decoder = torch.nn.GRU(hidden_size, hidden_size, \n",
    "                                    num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.head = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, encoder_sequence, decoder_sequence):\n",
    "        \n",
    "        encoder_embed = self.embedding_layer(encoder_sequence)\n",
    "        decoder_embed = self.embedding_layer(decoder_sequence)\n",
    "        \n",
    "        encoder_hidden, encoder_mem = self.encoder(encoder_embed)\n",
    "        \n",
    "        decoder_embed = torch.cat([encoder_hidden[:, -1, :].unsqueeze(1), decoder_embed], dim=1)\n",
    "        \n",
    "        decoder_hidden, _ = self.decoder(decoder_embed, encoder_mem)\n",
    "        \n",
    "        prediction = self.head(decoder_hidden)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleSequence2Sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSequence2Sequence(\n",
       "  (embedding_layer): Embedding(16000, 256, padding_idx=0)\n",
       "  (encoder): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (decoder): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (head): Linear(in_features=256, out_features=16000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for encoder_sequence, decoder_sequence in iterator:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(encoder_sequence, decoder_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 21, 16000])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = len(input_sequence)\n",
    "outputs = torch.zeros(batch_size, 20, self.vocab_size).to(self.device)\n",
    "\n",
    "for batch_element_index in range(batch_size):\n",
    "\n",
    "    target_tensor = output_sequence[batch_element_index, :]\n",
    "\n",
    "    decoder_input = torch.tensor([[self.sos_idx]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden[:,batch_element_index,:]\n",
    "\n",
    "    encoder_output_in_current_index = encoder_output[batch_element_index,:,:]\n",
    "\n",
    "    for di in range(20):\n",
    "\n",
    "        decoder_output, decoder_hidden = self.decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output_in_current_index)\n",
    "\n",
    "        outputs[batch_element_index,di,:] = decoder_output\n",
    "\n",
    "        if target_tensor[di] != self.pad_idx:\n",
    "            decoder_input = torch.tensor([target_tensor[di]], device=device).unsqueeze(1)\n",
    "        else:\n",
    "            break\n",
    "return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a46c87609e5423abade8569a2fd691d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=1286.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "pad_idx = tokenizer.vocab().index(\"<PAD>\")\n",
    "eos_idx = tokenizer.vocab().index(\"<EOS>\")\n",
    "sos_idx = tokenizer.vocab().index(\"<BOS>\")\n",
    "# Size of embedding_dim should match the dim of pre-trained word embeddings!\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "vocab_size = len(tokenizer.vocab())\n",
    "model = My_seq2seq_attention(embedding_dim,\n",
    "                 hidden_dim, \n",
    "                 vocab_size, \n",
    "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=1.0e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "train_losses = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, epoch)\n",
    "#     print (train_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    if min(train_losses) == train_loss and len(train_losses) > 1:\n",
    "        torch.save(model.state_dict, \"best_seq2seq_attention\")\n",
    "        torch.save(optimizer.state_dict, \"best_Adam_state_dict_attention\")\n",
    "    \n",
    "    torch.save(model.state_dict, \"last_seq2seq_attention\")\n",
    "    torch.save(optimizer.state_dict, \"Adam_state_dict_attention\")\n",
    "    \n",
    "    #early stopping\n",
    "    validation_losses = []\n",
    "    test_loss = evaluate(model, validation_loader)\n",
    "    validation_losses.append(test_loss)\n",
    "    \n",
    "    if len(validation_losses) > 1 and validation_losses[epoch] > validation_losses[epoch-1]:\n",
    "        print(\"stop\")\n",
    "        break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
